

# 三、经典神经网络

既然我们已经准备好了我们的图像数据，现在是时候利用我们所学的知识来构建一个经典的或密集的神经网络了。在本章中，我们将讨论以下主题:

*   首先，我们将看看经典的密集神经网络及其结构。
*   然后，我们将讨论激活函数和非线性。
*   当我们开始实际分类时，我们需要另一个数学工具，`softmax`。我们将在本章后面讨论为什么这很重要。
*   我们将查看训练和测试数据，以及`Dropout`和`Flatten`，它们是新的网络组件，旨在使网络更好地工作。
*   然后，我们将看看机器学习者实际上是如何解决的。
*   最后，我们将学习超参数和网格搜索的概念，以便尽可能地微调和构建最佳神经网络。

让我们开始吧。



# 经典密集神经网络的比较

在这一节中，我们将着眼于经典或密集神经网络的实际结构。我们将从一个样本神经网络结构开始，然后我们将扩展它来构建一个可视化的网络，你将需要它来理解 MNIST 数字。最后，我们将学习张量数据是如何实际插入网络的。

让我们先来看看密集神经网络的结构。使用网络包，我们将绘制一个神经网络的图片。下面的屏幕截图显示了我们正在设置的三个层，即输入层、激活层和输出层，并将它们完全连接起来:

![](assets/9ef65230-feb4-46b2-8bee-ff869a36568d.png)

三层神经网络

这就是中间这两个循环的作用。他们在每一次输入和每一次激活之间，然后是每一次激活和每一次输出之间设置了一个边界。这就是密集神经网络的定义:所有输入和所有激活，所有激活和所有输出之间的完全连接。如你所见，它生成的图片连接非常紧密，因此得名！

现在，让我们将它扩展到两个维度，一个 28 x 28 像素的网格(这是输入网络)，然后是一个 28 x 28 像素的激活网络，学习将在这里进行。最终，我们将登陆`10`职位分类网络，在那里我们将预测输出数字。从下面截图中的深色互连线可以看出，这是一个非常密集的结构:

![](assets/06a89c6e-c757-454e-af07-43488b7bb4a6.png)

二维网络

事实上，它是如此的密集，以至于很难看到单独线条的边缘。这些线路是网络内部进行数学运算的地方。激活函数，将在下一节中介绍，是沿着每一条线发生的数学运算。由此我们可以看出，张量和网络之间的关系相对简单:二维输入网格(在本图中是像素)是我们在上一节中了解的二维编码数据的位置。在网络内部，数学运算(通常是点积后跟激活函数)是将一个图层连接到另一个图层的线。



# 激活和非线性

我们将讨论为什么非线性很重要，然后我们将看看两个最常用的非线性函数的一些可视化形式:`sigmoid`和`relu`。

因此，非线性可能听起来像一个复杂的数学概念，但你基本上需要知道的是，它不是直线运动的。这使得神经网络能够学习更复杂的形状，而这种对网络结构内部复杂形状的学习正是神经网络和深度学习实际学习的内容。

那么，我们来看看`sigmoid`函数:

![](assets/cc268de7-1528-421d-8da7-1f54973a0fde.png)

Sigmoid 函数

这是一个范围从 0 到 1 的 S 曲线。它实际上是由 e 的指数和比值构成的。现在，好消息是，你实际上永远不需要编写你在这里看到的数学代码，因为当我们想在 Keras 中使用`sigmoid`时，我们只需通过名称`sigmoid`来引用它。

现在，我们来看看`relu`。`relu`非线性函数在技术上是一种非线性函数，因为当它小于零时，它是一条直线:

![](assets/de639205-919b-4d03-be0e-f915e54a8e61.png)

ReLu 非线性函数—小于零

当它大于零时，也是一条直线。但是两者的组合，零之前的平坦部分和零之后的角度一起，不形成一条直线:

![](assets/1de4e2f9-dc7b-4d8c-8040-b44321338c1e.png)

ReLu 非线性函数—大于零。

因为它是一个非常恒定的函数，当在计算机内部执行时，这在数学上是有效的，所以你会看到`relu`在许多生产神经网络模型中使用，只是因为它计算更快。但是`relu`函数的技巧，正如我们在前一章讨论规范化时所了解到的，是它们可以生成大于 1 的值，因此在构建神经网络时，通常需要各种技巧和技术，包括规范化和创建更多层，以使`relu`函数表现良好。

机器学习中发生的很多事情都涉及到反复计算这些`relu`和`sigmoid`函数的输入。

机器学习模型可能有数百、数千甚至数百万个单独的数字参数通过`relu`或`sigmoid`运行。

有大量的数学在进行，因此大量非线性的交互允许机器学习者在概念上围绕答案绘制高维数学形状。



# Softmax

在本节中，我们将了解称为`softmax`的输出激活功能。我们将看看它与输出类的关系，以及学习`softmax`如何生成概率。

我们来看看吧！当我们构建一个分类器时，神经网络将输出一堆数字，通常是一个数组，其中一个槽对应于我们的每个类。在我们现在看到的模型中，它是从 0 到 9 的数字。`softmax`所做的是将一大堆数字平滑成一组概率分数，这些分数的总和为 1:

![](assets/29f41dc2-2ebb-4547-bf56-396668a353b7.png)

一堆数字

这很重要，这样你才能知道哪个答案最有可能。因此，作为一个我们可以用来理解`softmax`的例子，让我们看看我们的值数组。我们可以看到有三个值。假设神经网络输出为`1`、`2`和`5`。我们正试着把这些分成红色、绿色和蓝色三类。现在，我们通过`softmax`运行它，我们可以看到概率得分。你可以清楚地看到，它应该是蓝色的，这用概率表示。读出`softmax`的方法是使用`argmax`。您查看具有最高值的单元格，并提取该索引作为您的预测类。但是如果你看看实际的数字——`1`、`2`和`5`——你可以看到这些加起来是 8，但是`5`的输出概率是`0.93`。那是因为`softmax`与指数一起工作。不仅仅是数字的线性组合，比如 5 除以 8，然后说 5/8 是属于那个类的概率。我们在这里说的是，最强的信号将会支配较弱的信号，这个指数实际上将会超过具有较高值的类的概率，以便当事物相对接近时，你的神经网络在分类中更有效。请记住，对于一个实际的神经网络，您不会输出漂亮的`1`、`2`和`5`数字，您将输出相对较小的十进制数字，例如`0.00007`，非常小的浮点数，然后我们需要能够将它们分成类。

现在你可能会想，考虑到你可以很容易地从数字`1`、`2`和`5`中判断出`5`是最大值，我们为什么要为此费心。这个想法是，如果你有以概率表示的东西，你可以模拟信心。在某种意义上，你可以在模型之间分享分数，并且知道你的模型实际上有多自信。另外，不同的型号在不同的范围内会产生不同的数字。仅仅因为你在你尝试的第一个模型中使用了`1`、`2`或`5`，这并不意味着它们在另一个模型中具有相同的相对值。因此，将它们分解成概率可以让你进行比较。现在，随着数学问题的解决，我们可以开始考虑构建实际的神经网络了。好消息是你实际上不需要记住或知道我们刚才列出的数学。你只需要记住数学的名称，因为在 Keras 中，你用一个简单的名字来引用激活函数。



# 培训和测试数据

在本节中，我们将了解如何获取培训和测试数据。我们将查看实际数据的加载，然后我们将重新讨论规范化和一次性编码，然后我们将快速讨论为什么我们实际上使用训练和测试数据集。

在本节中，我们将学习上一章中关于准备图像数据并将其压缩成几行代码的内容，如下面的屏幕截图所示:

![](assets/aba5c3c6-2bc3-43bb-b251-294ed35bcdf3.png)

加载数据

我们加载训练和测试数据以及训练和测试输出。然后，我们归一化，也就是除以最大值，我们知道这个值是`255`。然后，我们将输出变量分解成分类编码，或者说是一次性编码。对于我们的训练和测试数据集，我们以完全相同的方式做这两件事(标准化和一键编码)。在我们试图在机器学习模型中使用数据之前，我们的数据必须以相同的方式准备好，这一点很重要。这里有一个关于形状的快速注释。注意，训练数据(`x`和`y`)具有相同的初始编号:

![](assets/7b333062-bed5-400c-9745-34d7cdc2d593.png)

正在加载。形状(训练)

在这两种情况下，第一个维度都是`60000`，但是看看第二个和第三个维度(`28`和`28`)——这是输入图像的大小——以及`10`图。嗯，这些不一定要完全匹配，因为当我们通过模型运行时，我们所做的是将数据从`28`、`28`维度转换到`10`维度。

另外，看检测数据。可以看到是第一维度的`10000`(`28`、`28`，然后是第二维度的`10000`、`10`，如下截图所示:

![](assets/ea5329a4-d888-4df8-99f8-980ca7330c32.png)

正在加载。形状(测试)

这些维度以适当的方式匹配非常重要。因此，对于一个训练集，第一个维度必须匹配您的`x`和`y`值(您的输入和输出)，并且在您的测试集上，同样的事情也必须是正确的。但是还要注意，第二维度和第三维度`28`和`28`对于训练和测试数据都是相同的，并且`10`(输出维度)对于测试和训练数据都是相同的。在准备信息时，没有将这些数据集对齐是最常见的错误之一。但是为什么呢？！一句话:**过拟合**。

过度拟合本质上就是你的机器学习模型记忆了一组输入。您可以把它想象成一个非常复杂的哈希表，它用大量的数字对输入和输出映射进行了编码。但是对于机器学习，我们不想要哈希表，即使我们可以很容易地拥有一个。相反，我们希望有一个模型能够处理未知的输入，然后预测适当的输出。测试数据代表那些未知的输入。当你通过训练数据训练你的模型，并拿出测试数据时，测试数据就在那里，让你验证你的机器学习模型可以处理和预测它从未见过的数据。

好了，现在我们已经加载了我们的训练和测试数据，我们将继续学习`Dropout`和`Flatten`，并组装一个实际的神经网络。



# 辍学和扁平化

在本节中，我们将实际构建神经网络模型，并使用`Dropout`和`Flatten`来创建一个完整的神经网络。

我们将从使用功能 Keras 模型实际组装神经网络开始，查看输入和层堆栈，以便端到端地组装神经网络。然后，我们来解释为什么会有`Dropout`和`Flatten`，它们对你的模型有什么影响。最后，我们将展示一个模型摘要:这是一种可以可视化机器学习模型中参数和层的总数的方法。

这里，我们使用的是 Keras 的功能模型。您可以将神经网络视为一系列层，每一层都由一个函数定义。该函数传递一组参数来配置层，然后您将它作为参数传递给网络中的前一层，以将它们链接在一起。如下面的截图所示，这个微小的代码块实际上是一个完整的神经网络:

![](assets/b11f9fe4-535e-4e2f-9a4d-f9943449ba69.png)

Keras 的功能模型

我们从一个输入层开始，它的形状与我们的一个输入样本相同。在我们的例子中，我们选择了一个训练图像，我们从前面的课程中知道它的尺寸为 28x28 像素。现在，我们将它传递给一个堆栈。密集层之后是`dropout_1`，密集层之后是`dropout_2`，我们最终将其转化为`softmax`激活，将其移交给输出层。然后，我们将这些作为输入和输出组合到我们的模型中。然后，我们打印`summary`，看起来像这样:

![](assets/d4a8fa97-4247-4cd4-9752-beaa163ae9df.png)

模型摘要输出

因此，您可以从这里看到，参数最初被传递给层，然后层本身被传递以形成一个链。那么，这些`Dropout`和`Flatten`层呢？`Dropout`参数本质上是一个技巧。当我们设置`Dropout`参数(这里是`0.1`)时，我们告诉神经网络在每个训练周期中随机断开 10%的激活。这是让神经网络学会归纳；这才是真正的学习，而不是简单的记忆输入的数据。`Flatten`层处理尺寸。因为我们有一个 28x28 像素的二维输入图像，所以我们使用`Flatten`将它转换成一个长的一维数字串用于`784`。这被馈送到输出`softmax`层。

打印出模型的概要是计算参数大小和尺寸的好方法。这最终成为使用 Keras 的一个更棘手的部分，例如当您有一组输入样本时——在我们的例子中是 28x28 图像——您需要在到达`softmax`时将它们转换成一个由十个可能的输出值组成的数组。当我们通过每一层时，你可以看到形状是如何变化的。最后，`Flatten`将它转化为每个样本的一个维度，然后再转化为一个包含十个可能输出值的维度。

好了，现在是运行模型的时候了。现在我们已经了解了如何将模型放在一起，包括`Dropout`和`Flatten`层，我们将继续讨论解算器，这是我们用来实际执行机器学习模型的工具。



# 解决者（solver 的复数形式）

在本节中，我们将设置学习和优化功能，编译模型，使其适合训练和测试数据，然后实际运行模型，并观看动画，了解对损耗和精度的影响。

在下面的截图中，我们正在用`loss`、`optimizer`和`metrics`编译我们的模型:

![](assets/9e82cb5f-a0d4-4629-b043-3a12d45c1347.png)

编译模型

`loss`函数是一个数学函数，告诉`optimizer`它做得有多好。一个`optimizer`函数是一个数学程序，它搜索可用的参数以最小化`loss`函数。`metrics`参数是你的机器学习模型的输出，应该是人类可读的，这样你就可以了解你的模型运行得有多好。现在，这些`loss`和`optimizer`参数充满了数学。总的来说，你可以把它当作一本食谱。当你用 Keras 运行机器学习模型时，你应该有效地选择`adam`(这是默认的)。就`loss`函数而言，当你处理分类问题时，比如 MNIST 数字，你应该使用分类交叉熵。这种食谱式的配方应该对你有好处。

现在，我们将准备用我们的`x`训练数据(由实际的 MNIST 数字图像组成)和`y`训练参数(由 0 到 9 个分类输出标签组成)来拟合模型。我们这里有一个新概念是`batch_size`。这是每次执行循环的图像数量。通常，这受到可用内存的限制，但是较小的批处理大小(32 到 64)通常性能更好。还有这个奇怪的词怎么样:纪元。Epochs 只是指循环的次数。例如，当我们说八个时期时，我们的意思是机器学习模型将在训练数据上循环八次，并将使用测试数据来查看模型变得有多精确八次。正如您在下面的屏幕截图中看到的那样，由于模型重复查看相同的数据，因此准确性会提高:

![](assets/9395b3ec-e56e-484e-a1b9-ffa61aa0871f.png)

模型运行

最后，我们来看验证数据，也称为测试数据。这实际上是用来计算精度的。在每个时期结束时，模型被部分训练，然后测试数据通过模型运行，生成一组试验预测，用于对准确性进行评分。机器学习涉及到人类大量的等待。我们将继续前进，跳过每个时代的进程；当您运行这些示例时，您将有很多机会看到这些进度条自己成长。

现在，让我们来谈谈前面的输出。随着进度条的增长，您可以看到它正在运行的样本图像的数量。但是还有`loss`函数和`metrics`参数；在这里，我们使用的是准确性。所以，反馈给学习者的`loss`功能，这就是机器学习真正的学习方式；它试图通过迭代设置模型内部的数值参数来最小化`loss`，以便让`loss`数下降。准确性就在那里，这样你就能明白发生了什么。在这种情况下，精确度代表模型猜测正确数字的频率。所以，就像把这当成一本食谱一样，分类交叉熵是你总是想要有效地用于像这样的分类问题的`loss`函数，而`adam`是学习算法，是最明智的默认选择；`accuracy`是一个很棒的输出`metrics`，你可以用它来查看你的模型运行的有多好。



# 超参数

在这一节中，我们将探索超参数，即机器无法完全学习的参数。

我们还将介绍可训练参数(求解器学习的参数)、不可训练参数(模型中不需要训练的附加参数)，最后是超参数(传统求解器不学习的参数)。

在我们的*模型摘要输出*截图中，请注意截图底部突出显示的代码部分中可训练参数的数量。这是我们的模型中包含的单个浮点数的数量，我们的`adam`优化器，结合我们的分类交叉熵`loss`函数，将探索这个数量，以便找到可能的最佳参数值。所以，这个可训练的参数数是我们的`optimizer`函数学习的唯一一组数。然而，在这段代码和前面的截图中还有许多其他的数字。这些不可训练的参数呢？在我们当前的模型中，没有不可训练的参数。然而，Keras 中不同种类的图层可能具有常量值，因此它们将显示为不可训练的。同样，这仅仅意味着不需要对它们进行训练，并且我们的`optimizer`函数不会试图改变它们的值。

那么，什么是超参数？很简单，超参数是一个值——一个参数——在模型本身之外。所以最简单的超参数就是实际的模型结构。在这种情况下，我们创建层的次数是一个超参数，层的大小是一个超参数，我们在密集层中选择的`32`单位是一个超参数，`0.1` dropout 设置是一个超参数，甚至激活函数本身——比如说选择`relu`而不是`sigmoid`——也是一个超参数。现在你可能在想，*等一下，我不得不在这里选择一大堆参数；* *我以为机器应该在学习*。确实是！然而，问题在于`optimizer`无法学习我们需要知道的所有东西来构建一个最佳模型。



# 网格搜索

在本节中，我们将探索网格搜索。

我们将讨论优化与网格搜索，设置模型生成器功能，设置参数网格，进行交叉验证网格搜索，最后，报告网格搜索的结果，以便我们选择最佳模型。

那么，为什么从根本上说，这里有两种不同的机器学习活动呢？嗯，优化通过来自`loss`函数的反馈来求解参数:这是高度优化的。具体来说，求解器不必为了工作而尝试每个参数值。它利用偏导数的数学关系，沿着所谓的**梯度**移动。这使得它在数学上走下坡路来寻找正确的答案。

网格搜索并不那么聪明。其实完全是蛮力。当我们谈论进行网格搜索时，我们实际上是在谈论探索参数值的每一种可能的组合。网格搜索源于这样一个事实，即两组不同的参数形成了一个棋盘或网格，网格搜索涉及运行每个方格中的值。因此，如你所见，网格搜索的效率远远低于优化。那么，你为什么要用网格搜索呢？嗯，当你需要学习优化器无法解决的参数时，你可以使用它，这是机器学习中的一个常见场景。理想情况下，你会有一个算法，解决所有的参数。然而，目前还没有这样的算法。

好了，让我们来看一些代码:

![](assets/a1662142-abb1-4c6e-9f87-ae633375d6b8.png)

模型生成函数和构想两个超参数

我们将使用 scikit-learn，这是一个经常与 Keras 和其他机器学习软件一起使用的工具包，用于进行网格搜索和分类报告，这将告诉我们最佳模型。然后，我们还将导入 Keras 的`KerasClassifier`包装器，使其与`scikit_learn`兼容。

所以现在，让我们关注一个模型生成函数，设想两个超参数。其中一个是`dropout`，另一个是每个密集隐藏层中的单元数。因此，我们在这里构建一个名为`dense_model`的函数，它接受`units`和`dropout`，然后像之前一样计算我们的网络。但是不是硬编码的`32`或`0.1`(例如)，而是实际的参数将被传入，它将为我们编译模型，然后将该模型作为输出返回。这一次，我们使用顺序模型。以前，当我们使用 Keras 功能模型时，我们将我们的层一个接一个地链接在一起。对于顺序模型，它更像一个列表:从顺序模型开始，一层一层地添加，直到顺序模型本身为您形成了链。现在是超参数网格。这是我们指出网格搜索相对于优化器的一些缺点的地方。您可以在前面的屏幕截图中看到我们选择的值。为了让事情运行得更快，我们将做一个 epoch，并且我们将保持一个恒定的`64`图像的`batch_size`，这些图像将在`32`、`64`和`128`隐藏单元之间变化，并且会丢失`0.1`、`0.2`和`0.4`。网格搜索有一个很大的缺点:你在这里看到的超参数是唯一会被执行的——网格搜索不会探索中间的超参数值。

现在，我们设置我们的`KerasClassifier`，将我们刚刚创建的模型构建函数交给它，并将`verbose`设置为`0`，以隐藏每次 Keras 运行的进度条。然后，我们设置一个计时器；我想知道这需要多长时间。现在，我们建立一个交叉验证的网格搜索。对于它的估计器，我们给它我们的模型，也就是我们的`KerasClassifier`包装器，和我们的`grid`参数(参见前面的超参数)，我们说`cv=6`，意思是将数据(训练数据)分成六个不同的部分，然后交叉验证。在`5`进行训练，并使用六分之一的时间进行验证，并反复重复，以搜索最佳超参数值。另外，将`verbose`设置为`4`，这样我们可以看到很多输出。现在 Keras 已经运行了很多，我们调用`fit`函数从我们的`x`训练数据(同样，这些是我们的输入图像)到我们的`y`训练数据(这些是从数字 0 到 9 的标签),然后打印出我们的最佳结果。注意，我们实际上还没有接触到我们的测试数据；我们一会儿将使用它来对网格搜索报告的最佳模型的值进行评分。

现在，我们测试结果。这就是我们使用`argmax`的地方。同样，这是一个查看数组并挑选出其中具有最大值的索引的函数。实际上，这将一个由十个独热编码值组成的数组转换成了一个数字，这个数字就是我们要预测的数字。然后，我们使用一个分类报告，它将打印出`x`网格，向我们显示一个数字被正确预测的频率与被预测的数字总数的比较。

好了，前面代码的输出如下:

![](assets/f5a5f6f4-fffe-470a-aa84-3ae2243f5aa1.png)

输出—打印分数

我们正在探索超参数网格中的每个参数，并打印出分数。这就是网格搜索搜索最佳可用模型的方式。当我们都完成后，一个单一的模型将被选中。在这种情况下，它是隐藏单元数量最多的一个，我们将通过分类报告来评估该模型使用我们的测试数据的情况。

在下面的屏幕截图中，您可以看到打印输出包含我们已经识别的每一个数字，以及精度(我们正确分类该数字的时间百分比)和召回率(我们实际召回的数字的数量):

![](assets/94516ea5-95e2-4dd1-9ed4-ba45b84affaa.png)

输出—最终得分

你可以看到我们的分数相当不错:总体准确率为 96%。



# 摘要

在这一章中，我们实际上涵盖了大量的材料。我们看到了经典或密集神经网络的结构。我们学习了激活和非线性，我们还学习了`softmax`。然后，我们设置测试和训练数据，并学习如何用`Dropout`和`Flatten`构建网络。我们还学习了所有关于解算器的知识，或者机器学习实际上是如何学习的。然后，我们探索超参数，最后，我们用网格搜索微调我们的模型。

在下一章中，我们将利用我们所学的知识，改变我们的网络结构，构建一个所谓的**卷积神经网络** ( **CNN** )。