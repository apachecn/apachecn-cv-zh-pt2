# 九、OpenCV 简介

自从第一章介绍树莓派以来，我们已经走过了很长的路。至此，您已经了解了 Pi 和 Arduino。你已经学会了如何对两块电路板进行编程。你和传感器和马达一起工作过。你已经制造了你的机器人，并给它编了程序，让它四处漫游并跟随一条线。

然而，老实说，你并没有真正需要树莓派的功能。事实上，这有点阻碍。你用机器人做的所有事情——漫游和循线，没有 Pi 你也可以用 Arduino 做得很好。现在是时候展示圆周率的真正力量，并了解为什么你想在你的机器人使用它。

在这一章中，我们将做一些你不能单独用 Arduino 做的事情。我们将连接一个简单的网络摄像头，并开始使用通常所说的计算机视觉。

## 计算机视觉

计算机视觉是允许计算机分析图像并提取有用信息的算法的集合。它应用广泛，并迅速成为日常生活的一部分。如果你有一部智能手机，你很可能至少有一个应用使用计算机视觉。大多数新的中高端相机都内置了面部检测功能。脸书使用计算机视觉进行面部检测。计算机视觉被运输公司用来跟踪他们仓库里的包裹。当然，它被用于机器人导航、物体探测、物体回避和许多其他行为。

这一切都始于一幅图像。计算机分析图像以识别线条、角落和大范围的颜色。这个过程被称为特征提取，它是几乎所有计算机视觉算法的第一步。一旦这些特征被提取出来，计算机就可以将这些信息用于许多不同的任务。

面部识别是通过将特征与包含面部特征数据的 XML 文件进行比较来完成的。这些 XML 文件被称为级联。它们可用于许多不同类型的对象，而不仅仅是面部。同样的技术也可以用于物体识别。您只需向应用提供您感兴趣的对象的特性信息。

计算机视觉也包含视频。运动跟踪是计算机视觉的一个常见应用。为了检测运动，计算机比较来自静止摄像机的单个帧。如果没有运动，特征将不会在帧之间改变。因此，如果计算机识别出帧之间的差异，最有可能的是运动。基于计算机视觉的运动跟踪比红外传感器更可靠，如第 [8](08.html) 章中讨论的 PIR 传感器。

计算机视觉的一个令人兴奋的最新应用是增强现实。从视频流中提取的特征可用于识别表面上的独特图案。因为计算机知道图案，所以它可以很容易地计算出表面的角度。然后在图案上叠加 3D 模型。这个 3D 模型可以是物理的东西，比如建筑物，也可以是具有二维文本的平面对象。建筑师用这种技术向客户展示一座建筑在地平线上的样子。博物馆用它来提供更多关于展览或艺术家的信息。

所有这些都是现代环境下计算机视觉的例子。但是应用的列表太大了，无法在这里深入讨论，而且还在不断增加。

### 开放计算机视觉

就在几年前，计算机视觉对业余爱好者来说还不太容易。它需要大量繁重的数学运算，甚至更繁重的处理。计算机视觉项目通常使用笔记本电脑完成，这限制了它的应用。

OpenCV 已经存在一段时间了。1999 年，英特尔研究院建立了一个促进计算机视觉发展的开放标准。2012 年，它被非营利组织 OpenCV 基金会接管。你可以在他们的网站上下载最新版本。然而，要让它在 Raspberry Pi 上运行还需要一点额外的努力。我们很快就会谈到这一点。

OpenCV 是用 C++原生编写的；但是，它可以在 C、Java 和 Python 中使用。我们对 Python 实现感兴趣。因为我们的电机控制器库与 Python 3 不兼容，所以我们需要安装 OpenCV for Python 2.7。

#### 安装 OpenCV

我们将在树莓派 上安装 OpenCV。你要确保你的树莓皮插在充电器上，而不是电池组上，给自己足够的时间来安装。我们将从源代码编译 OpenCV，这意味着我们将从互联网上下载源代码，并直接在 Pi 上构建它。需要注意的是，尽管这个过程并不困难，但确实需要很长时间，并且需要输入许多 Linux 命令。我通常在晚上开始这个过程，让最终的构建运行一整夜。

1.  登录您的树莓派。
2.  在 Pi 上打开一个终端窗口。
3.  确保 Raspberry Pi 已更新。

    ```py
    sudo apt-get update
    sudo apt-get upgrade
    sudo rpi-update
    sudo reboot

    ```

4.  这些命令安装了构建 OpenCV 的先决条件。

    ```py
    sudo apt-get install build-essential git cmake pkg-config
    sudo apt-get install libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev
    sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev
    sudo apt-get install libxvidcore-dev libx264-dev
    sudo apt-get install libgtk2.0-dev
    sudo apt-get install libatlas-base-dev gfortran

    ```

5.  下载 OpenCV 源代码和 OpenCV 贡献的文件。贡献的文件包含了许多 OpenCV 主发行版中没有的功能。

    ```py
    cd ~
    git clone https://github.com/Itseez/opencv.git
    cd opencv
    git checkout 3.1.0
    cd ~
    git clone https://github.com/Itseez/opencv_contrib.git
    cd opencv_contrib
    git checkout 3.1.0

    ```

6.  安装 Python 开发库和 pip。

    ```py
    sudo apt-get install python2.7-dev
    wget https://bootstrap.pypa.io/get-pip.py
    sudo python get-pip.py

    ```

7.  确保安装了 NumPy。

    ```py
    pip install numpy

    ```

8.  准备编译的源代码。

    ```py
    cd ~/opencv
    mkdir build
    cd build
    cmake -D CMAKE_BUILD_TYPE=RELEASE \
        -D CMAKE_INSTALL_PREFIX=/usr/local \
        -D INSTALL_C_EXAMPLES=OFF \
        -D INSTALL_PYTHON_EXAMPLES=ON \
        -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules \
        -D BUILD_EXAMPLES=ON ..

    ```

9.  现在让我们编译源代码。这部分需要一段时间。有些人试图利用树莓派 的 ARM CPU 中的所有四个核心。然而，我发现这很容易出错，而且它从来没有为我工作过。我的建议是咬紧牙关:让 Pi 决定要使用的内核数量，并让它运行。如果您想大胆尝试，您可以通过在下面的行

    ```py
    make

    ```

    中添加`–j4`开关来强制 Pi 使用四个内核
10.  如果您尝试了–j4 开关，但失败了，大约在第四个小时，请输入以下行:

    ```py
    make clean
    make

    ```

11.  编译好源代码后，现在就可以安装它了。

    ```py
    sudo make install
    sudo ldconfig

    ```

12.  通过打开 Python 命令行来测试安装。

    ```py
    python

    ```

13.  导入 OpenCV。

    ```py
    >>>import cv2

    ```

现在，您的 Raspberry Pi 上应该已经安装了 OpenCV 的运行版本。如果导入命令不起作用，您需要确定它没有安装的原因。互联网是您排除故障的指南。

### 选择摄像机

在我们真正让 OpenCV 在我们的机器人上工作之前，我们需要安装一个摄像头。Raspberry Pi 有几个选项:Pi 摄像头或 USB 网络摄像头。

Pi 摄像机直接连接到专门为其设计的端口。一旦连接上，您需要进入 raspi-config 并启用它。Pi 摄像头的优势在于它比 USB 摄像头快一点，因为它直接连接到电路板。它不通过 USB 串行总线。这让它有了一点优势。

大多数 Pi 摄像机都配有一根短的 6 英寸带状电缆。由于树莓派在我们机器人上的位置，这是不够的。可以订购更长的电缆。Adafruit 有几个选择。但是，对于这个项目，我们将使用一个简单的网络摄像机。

USB 摄像头在任何电子产品零售商处都很容易买到。网上也有很多选择。对于这个基本的应用，我们不需要任何特别健壮的东西。任何能提供像样图像的相机都可以。拥有高分辨率也不是问题。由于我们是在 Raspberry Pi 的有限资源下运行相机，较低的分辨率实际上会有助于性能。记住，OpenCV 逐像素分析每一帧。图像中的像素越多，需要处理的事情就越多。

对于我的机器人，我选择了直播！Cam Sync HD by Creative(见图 [9-1](#Fig1) )，这是一款基本的高清网络摄像头，内置麦克风，通过标准 USB 2 端口操作。这个项目我们不需要麦克风，但将来可能会需要。它可以捕捉 720p 高清视频，这对我们的机器人来说可能有点多，但如果性能受到影响，我总是在软件中降低分辨率。

![A457480_1_En_9_Fig1_HTML.jpg](A457480_1_En_9_Fig1_HTML.jpg)

图 9-1

Creative Live! Cam Sync HD

### 安装摄像机

大多数网络摄像头都安装在显示器的顶部。他们通常有一个折叠夹，为监视器上的摄像机提供支撑。不幸的是，这些夹子通常是作为相机机身的一部分模制而成的，如果不损坏相机就无法拆除。这当然适用于生活！凸轮同步。所以，再一次，一点点创造力开始发挥作用。

我用来安装传感器的支架以 45 度角从机器人的前面脱落。为了让自己轻松一点，我选择不在相机支架上钻孔。相反，我使用我的可靠的安装胶带和一对安装支架。当我爬上去的时候，我想让它爬得相当高，并且稍微向下。这个想法是给它最好的视角，以及任何直接在它前面的物体。我还希望镜头尽可能靠近中心轴，以使软件方面的事情更简单。图 [9-2](#Fig2) 显示安装摄像头后的机器人。

![A457480_1_En_9_Fig2_HTML.jpg](A457480_1_En_9_Fig2_HTML.jpg)

图 9-2

Camera mounted on robot

## OpenCV 基础知识

OpenCV 有许多功能。它拥有 500 多个库和数千种功能。这是一个非常大的主题——太大了，一章无法涵盖。我将讨论在您的机器人上执行一些简单任务所需的基础知识。

我说简单的任务。这些任务非常简单，因为 OpenCV 抽象了后台发生的大量数学运算。当我考虑几年前机器人爱好的状态时，我发现即使是最基本的东西也能轻易获得，这真是令人惊讶。

目标是在本章结束时建造一个能够识别球并向球移动的机器人。我介绍的函数将帮助我们实现这个目标。我强烈建议花时间浏览 OpenCV 网站上的一些教程( [`https://opencv.org`](https://opencv.org) )。

要在 Python 代码中使用 OpenCV，您需要导入它。而且，当您这样做时，您可能还需要导入 NumPy 库。NumPy 增加了许多数学和数字处理功能，使得使用 OpenCV 更加容易。所有与图像相关的代码都应该以此开头:

```py
import cv2
import numpy as np

```

在本章的代码讨论中，我假设这已经完成了。以`cv2`为前缀的函数是 OpenCV 函数。如果以`np`为前缀，则为 NumPy 函数。如果你想扩展你在这本书里读到的内容，这一点很重要。OpenCV 和 NumPy 是两个独立的库，但是 OpenCV 经常使用 NumPy。

### 使用图像

在本节中，您将学习如何从文件中打开图像，以及如何从摄像机中捕捉实时视频。然后，我们将看看如何处理和分析这些图像，从中获取有用的信息。具体来说，我们将研究如何识别特定颜色的球，并跟踪它在帧中的位置。

但是首先，我们有一个先有鸡还是先有蛋的问题。我们需要在所有的练习中看到我们图像处理的结果。为此，我们需要从如何显示图像开始。这是我们会广泛使用的东西，而且非常容易使用。但我想确保在你学习如何捕捉图像之前，我先覆盖它。

#### 显示图像

在 OpenCV 中显示图像实际上非常容易。`imshow()`功能提供了这一功能。该函数适用于静态图像和视频图像，并且两者之间的实现不会改变。`imshow()`功能打开一个新窗口显示图像。当您调用它时，您必须为窗口以及您想要显示的图像或框架提供一个名称。

这是关于 OpenCV 如何处理视频的重要一点。因为 OpenCV 将视频视为一系列独立的帧，所以几乎所有用于修改或分析图像的功能都适用于视频。这显然包括`imshow()`。

如果我们想显示一个加载到`img`变量中的图像，它看起来会像这样:

```py
cv2.imshow('img', img)

```

在这个例子中，第一个参数是窗口的名称。它出现在窗口的标题栏中。第二个参数是保存图像的变量。显示视频的格式完全相同。我通常使用变量 cap 进行视频捕获，因此代码如下所示:

```py
cv2.imshow('cap', cap)

```

如您所见，代码是相同的。同样，这是因为 OpenCV 将视频视为一系列独立的帧。实际上，视频捕获依靠一个循环来连续捕获下一帧。所以从本质上来说，显示一个文件中的静止图像和一个相机中的单独帧是完全一样的。

还有一个元素要显示。为了实际显示图像，`imshow()`函数要求也调用`waitKey()`。`waitKey()`功能等待键盘按键按下指定的毫秒数。许多人用这个来捕捉一个退出键。除非我需要按键，否则我通常将它传递为零。

```py
cv2.waitKey(0)

```

我们在本章中广泛使用了`imshow()`和`waitKey()`。

### 捕捉图像

使用 OpenCV 所需的图像有几个来源，所有这些来源都是两个因素的变体:文件或相机，以及静止或视频。在大多数情况下，我们只关心来自摄像机的视频，因为我们使用 OpenCV 进行导航。但是所有的方法都有优点。

打开一个静止图像文件是学习新技术的一个很好的方法，尤其是当您正在处理计算机视觉的特定方面时。例如，如果您正在学习如何使用滤镜来识别某种颜色的球，那么使用由三个不同颜色的球组成的静止图像(除此之外别无其他)可以让您专注于特定的目标，而不必担心用于捕捉实时视频流的底层框架。哦，这是一个伏笔，如果你没有意识到这一点。

从用相机捕捉静止图像中学到的技术可以应用到真实环境中。它允许您通过使用包含真实世界元素的图像来改进或微调代码。

显然，捕捉现场视频是我们在机器人中使用的目标。实时视频流是我们用来识别我们的目标对象，然后导航到它。随着您的计算机视觉经验的增长，您可能会将运动检测或其他方法添加到您的清单中。由于机器人上的摄像头的目的是实时采集环境信息，因此需要实时视频。

文件中的视频对于学习过程也非常有用。您可能希望从机器人捕捉实时视频，并将其保存到文件中供以后分析。比方说，你正在利用一天中你能找到的任何空闲时间从事你的机器人项目。你可以随身携带你的笔记本电脑，但是携带一个机器人却是另一回事。因此，如果你从你的机器人那里录制视频，你可以在没有机器人陪伴的情况下进行你的计算机视觉算法。

请记住，Python 和 OpenCV 的一大优点是它们是抽象的，并且在很大程度上是平台独立的。所以，你在 Windows 机器上写的代码移植到你的 Raspberry Pi 上。

出差，并希望在酒店休息一段时间？去家人那里度假，偶尔需要离开一下？在午餐时间或课间溜进一点机器人程序？将录制的视频与 Python 和 OpenCV 的本地实例一起使用，并使用您的检测算法。当你回家后，你可以把代码转移到你的机器人上，并进行现场测试。

在本节中，我们使用前三种技术。我向您展示了如何保存和打开视频文件，但在大多数情况下，我们将使用静止图像来学习检测算法，使用实时视频来学习跟踪。

#### 打开图像文件

OpenCV 使得处理图像和文件变得非常容易，尤其是考虑到后台发生的事情使得这些操作成为可能。打开图像文件也不例外。我们使用`imread()`函数从本地存储器打开图像文件。`imread()`函数有两个参数:文件名和颜色类型标志。打开文件显然需要文件名。颜色类型标志决定了是以彩色还是灰度打开图像。

让我们打开并显示一个图像。我将使用三个彩色球的图像，这也是本章后面学习如何检测颜色的图像。如果您已经安装了 Python 和 OpenCV，这个练习可以在 Pi 或您的计算机上完成。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`open_image.py`。
3.  输入以下代码:

    ```py
    import cv2

    img = cv2.imread('color_balls_small.jpg')
    cv2.imshow('image',img)

    cv2.waitKey(0)

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`python open_image.py`并按回车键。

一个窗口打开，在白色背景上显示三个彩球的图像(参见图 [9-3](#Fig3) )。按任意键关闭它。

![A457480_1_En_9_Fig3_HTML.jpg](A457480_1_En_9_Fig3_HTML.jpg)

图 9-3

Three colored balls

由于 IDLE 与基于 Linux 的机器上的 GUI 系统的交互方式，如果您直接从 IDLE 运行代码，图像窗口将不会正常关闭。然而，通过从终端运行代码，我们没有这个问题。

#### 捕捉视频

用相机捕捉视频与打开文件略有不同。使用视频还有几个步骤。一个变化是我们必须使用一个循环来获得多个帧；否则，OpenCV 只会捕获单个帧，这不是我们想要的。通常使用开放的`while`回路。这会捕捉视频，直到我们主动停止它。

为了使测试更容易，我将球直接放在摄像机的前面(见图 [9-4](#Fig4) )。现在，我们只想捕捉图像。

为了从摄像机中捕捉视频，我们将创建一个`videoCapture()`对象，然后在一个循环中使用`read()`方法来捕捉帧。`read()`方法返回两个对象:一个返回值和一个图像帧。返回值只是一个验证读取成功或失败的整数。如果读取成功，则值为 1；否则，读取失败，并返回 0。为了防止导致代码出错的错误，可以测试读取是否成功。

![A457480_1_En_9_Fig4_HTML.jpg](A457480_1_En_9_Fig4_HTML.jpg)

图 9-4

Ball positioned in front of the robot for testing

我们关心图像帧。如果读取成功，则返回一个图像。如果不是，则返回一个空对象来代替它。由于空对象不能访问 OpenCV 方法，所以当您试图修改或操作图像时，您的代码将会崩溃。这就是为什么测试读取操作是否成功是个好主意。

##### 查看摄像机

在下一个练习中，我们将打开之前安装的摄像机来观看视频。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为 view_camera.py。
3.  输入以下代码:

    ```py
    import cv2
    import numpy as np

    cap = cv2.VideoCapture(0)

    while(True):
        ret,frame = cap.read()

        cv2.imshow('video', frame)
        if cv2.waitKey(1) & 0xff == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存脚本的工作文件夹。
7.  类型`sudo python view_camera.py`。

这将打开一个窗口，显示您的摄像机看到的内容。如果您使用远程桌面会话来处理 Pi，您可能会看到以下警告消息:Xlib:extension RANR missing on display:10。此消息意味着系统正在寻找 vncserver 中不包含的功能。可以忽略。

如果您担心视频图像的刷新率，请记住，当我们通过远程桌面会话运行几个窗口时，我们对 Raspberry Pi 的要求非常高。如果您连接显示器和键盘来访问 Pi，它会运行得更快。如果您在没有可视化的情况下运行视频捕获，它会运行得更快。

##### 录制视频

录制视频是查看摄像机的延伸。要录制，您必须声明您将使用的视频编解码器，然后设置将传入视频写入 SD 卡的`VideoWriter`对象。

OpenCV 使用 FOURCC 码来指定编解码器。FOURCC 是视频编解码器的四字符代码。你可以在 [`www.fourcc.org`](http://www.fourcc.org) 找到更多关于 FOURCC 的信息。

当创建`VideoWriter`对象时，我们需要提供一些信息。首先，我们必须提供文件名来保存视频。接下来，我们提供编解码器，然后是帧速率和分辨率。一旦创建了`VideoWriter`对象，我们只需使用`VideoWriter`对象的`write()`方法将每个从写到文件中。

让我们录制一些机器人的视频。我们将使用 XVID 编解码器写入一个名为`test_video.avi`的文件。我们将使用上一个练习中的视频捕获代码，而不是从头开始。

1.  在空闲 IDE 中打开 view_camera.py 文件。
2.  选择文件➤另存为并将文件另存为`record_camera.py`。
3.  更新代码。在下面，新行以粗体显示:

    ```py
    import cv2
    import numpy as np

    cap = cv2.VideoCapture(0)
    fourcc = cv2.VideoWriter_fourcc(*'XVID')
    vidWrite = cv2.VideoWriter('test_video.avi', \
                    fourcc, 20, (640,480))

    while(True):
        ret,frame = cap.read()

        vidWrite.write(frame)

        cv2.imshow('video', frame)
        if cv2.waitKey(1) & 0xff == ord('q'):
            break

    cap.release()

    vidWrite.release()

    cv2.destroyAllWindows()

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存脚本的工作文件夹。
7.  类型`sudo python record_camera.py`。
8.  让视频运行几秒钟，然后按 Q 结束程序并关闭窗口。

现在，您的工作目录中应该有一个视频文件。接下来，我们将看看从文件中读取视频。

代码中有几项需要注意。当我们创建`VideoWriter`对象时，我们以元组的形式提供了视频分辨率。这是整个 OpenCV 中非常常见的做法。此外，我们必须释放`VideoWriter`对象。这关闭了文件的写入。

##### 从文件中读取视频

从文件中回放视频与从摄像机中观看视频完全相同。唯一的区别是，我们提供要播放的文件的名称，而不是向视频设备提供索引。我们将使用`ret`变量来测试视频文件的结尾；否则，当没有更多的视频播放时，我们会得到一个错误。

在本练习中，我们将简单地回放我们在上一个练习中录制的视频。代码应该看起来非常熟悉。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`view_video.py`。
3.  输入以下代码:

    ```py
    import cv2
    import numpy as np

    cap = cv2.VideoCapture('test_video.avi')

    while(True):
        ret,frame = cap.read()

        if ret:
            cv2.imshow('video', frame)
        if cv2.waitKey(1) & 0xff == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存脚本的工作文件夹。
7.  类型`sudo python view_video.py`。

一个新窗口打开。它显示了我们在上一个练习中记录的视频文件。当到达文件结尾时，视频停止。按 Q 结束程序并关闭窗口。

### 图像转换

现在你知道了更多关于如何获取图像的知识，让我们来看看我们可以用它们做些什么。我们将看一些非常基本的操作。选择这些操作是因为它们将帮助我们达到跟踪球的目标。OpenCV 非常强大，它拥有比我在这里介绍的更多的功能。

#### 轻弹

很多时候，摄像机在项目中的摆放并不理想。经常，我不得不把相机倒过来装，或者因为这样或那样的原因，我需要翻转图像。

幸运的是，OpenCV 用`flip()`方法使这变得非常简单。`flip()`方法有三个参数:要翻转的图像、指示如何翻转的代码和翻转图像的目的地。最后一个参数仅在您想要将翻转的图像指定给另一个变量时使用，但是您可以在适当的位置翻转图像。

通过提供 flipCode，可以水平和/或垂直翻转图像。flipCode 为正、负或零。零水平翻转图像，正值垂直翻转图像，负值在两个轴上翻转图像。通常情况下，您会在两个轴上翻转图像，以有效地将其旋转 180 度。

让我们使用之前使用的三个球的图像来说明翻转帧。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`flip_image.py`。
3.  输入以下代码:

    ```py
    import cv2

    img = cv2.imread('color_balls_small.jpg')
    h_img = cv2.flip(img, 0)
    v_img = cv2.flip(img, 1)
    b_img = cv2.flip(img, -1)

    cv2.imshow('image', img)
    cv2.imshow('horizontal', h_img)
    cv2.imshow('vertical', v_img)
    cv2.imshow('both', b_img)

    cv2.waitKey(0)

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`python flip_image.py`并按回车键。

打开四个窗口，每个窗口都有不同版本的图像文件。按任意键退出。

#### 调整大小

您可以调整图像的大小。这有助于减少处理图像所需的资源。图像越大，需要的内存和 CPU 资源就越多。为了调整图像的大小，我们使用了`resize()`方法。这些参数是要缩放的图像、作为元组的所需维度以及插值。

插值是一种数学方法，用于确定如何处理像素的删除或添加。请记住，在处理图像时，您实际上是在处理一个多维数组，该数组包含组成图像的每个点或像素的信息。缩小图像时，您正在删除像素。当你放大一幅图像时，你是在增加像素。插值是发生这种情况的方法。

有三种插值选项。INTER_AREA 最适合用于归约。INTER_CUBIC 和 INTER_LINEAR 都适合放大图像，INTER_LINEAR 是两者中速度较快的。如果没有提供插值，OpenCV 使用 INTER_LINEAR 作为缩小和放大的缺省值。

三个球的图像目前为 800×533 像素。虽然它不是大号的，但我们会把它做小一点。让我们把两个轴的大小都设为当前大小的一半。为此，我们将使用 INTER_AREA 插值。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`resize_image.py`。
3.  输入以下代码:

    ```py
    import cv2

    img = cv2.imread('color_balls_small.jpg')
    x,y = img.shape[:2]
    resImg = cv2.resize(img, (y/2, x/2), interpolation = cv2.INTER_AREA)

    cv2.imshow('image', img)
    cv2.imshow('resized', resImg)

    cv2.waitKey(0)

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`python resize_image.py`并按回车键。

应该开了两扇窗。第一个有原始图像。第二个显示缩小的图像。按任意键关闭窗口。

### 使用颜色

颜色显然是处理图像的一个非常重要的部分。因此，它是 OpenCV 中非常重要的一部分。颜色可以做很多事情。我们将重点关注完成我们的最终目标所需的几个关键要素，即识别和用机器人追球。

#### 色彩空间

处理颜色的一个关键要素是颜色空间，它描述了 OpenCV 如何表达颜色。在 OpenCV 中，颜色由一系列数字表示。色彩空间决定了这些数字的含义。

OpenCV 的默认颜色空间是 BGR。这意味着每种颜色都由 0 到 255 之间的三个整数来描述，它们依次对应于三个颜色通道——蓝色、绿色和红色。表示为(255，0，0)的颜色在蓝色通道中具有最大值，绿色和红色都为零。这代表纯蓝色。鉴于此，(0，255，0)为绿色，(0，0，255)为红色。值(0，0，0)代表黑色，没有任何颜色，而(255，255，255)代表白色。

如果你过去曾与图形打交道，BGR 与你可能习惯的截然相反。大多数数字图形都是用 RGB 来描述的，即红色、绿色和蓝色。所以，这可能需要一点时间来适应。

有许多颜色空间。我们关心的是 BGR、RGB、HSV 和灰度。我们已经讨论了默认的颜色空间，BGR，和常见的 RGB 颜色空间。HSV 是色调、饱和度和值。色调代表 0 到 180 范围内的颜色。饱和度表示颜色从 0 到 255 有多白。值是从 0 到 255 的颜色与黑色的距离。如果饱和度和值都为 0，则颜色为灰色。饱和度和值 255 是色调的最亮版本。

色调有点复杂。它在 0 到 180 的范围内，0 和 180 都是红色。这就是记住色轮的重要性。如果 0 和 180 在红色空间中间的轮盘顶部相遇，当你围绕轮盘顺时针移动时，色相= 30 是黄色，色相= 60 是绿色，色相= 90 是蓝绿色，色相= 120 是蓝色，色相= 150 是紫色，色相= 180 将我们带回红色。

你最常遇到的是灰度。灰度就是它听起来的样子:图像的黑白版本。特征检测算法使用它来创建遮罩。我们用它来过滤物体。

要将图像转换到不同的色彩空间，可以使用`cvtColor`方法。它需要两个参数:图像和颜色空间常数。颜色空间常量内置于 OpenCV 中。分别是 COLOR_BGR2RGB、COLOR_BGR2HSV、COLOR_BGR2GRAY。你看到那里的模式了吗？如果您想从 RGB 颜色空间转换到 HSV 颜色空间，常量应该是 COLOR_RGB2HSV。

让我们把三个彩球的图像转换成灰度图像。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`gray_image.py`。
3.  输入以下代码:

    ```py
    import cv2

    img = cv2.imread('color_balls_small.jpg')
    grayImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    cv2.imshow('img', img)
    cv2.imshow('gray', grayImg)

    cv2.waitKey(0)

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`python gray_image.py`并按回车键。

这将打开两个窗口:一个显示原始彩色图像，另一个显示灰度图像。单击任意键退出程序并关闭窗口。

#### 颜色过滤器

对颜色进行过滤需要的代码非常少，但同时，这可能会有点令人沮丧，因为您通常不是在寻找特定的颜色，而是一个颜色范围。颜色很少是纯粹的，只有一种价值。这就是为什么我们希望能够在色彩空间之间转换。当然，我们可以在 BGR 寻找红色。但是要做到这一点，我们需要这三个值的具体范围。在所有色彩空间都是如此的情况下，通常在 HSV 空间中更容易调整到您需要的范围。

用于过滤特定颜色的策略相当简单，但是需要几个步骤，并且需要记住一些事情。

首先，我们将在 HSV 颜色空间中复制图像。然后，我们应用我们的过滤范围，使其成为自己的图像。为此，我们使用`inRange()`方法。它有三个参数:我们要应用滤镜的图像、下限值范围和上限值范围。`inRange`方法扫描提供的图像中的所有像素，以确定它们是否在指定的范围内。如果是，则返回 true 或 1；否则，它返回 0。这留给我们的是一个黑白图像，我们可以用它作为一个面具。

接下来，我们使用`bitwise_and()`方法应用蒙版。该方法获取两幅图像，并返回像素匹配的区域。因为这不是我们想要的，我们需要耍点小花招。出于我们的目的，`bitwise_and`需要三个参数:图像 1、图像 2 和一个遮罩。因为我们想要返回遮罩显示的所有内容，所以图像 1 和图像 2 都使用我们的原始图像。然后，我们通过指定遮罩参数来应用遮罩。因为我们忽略了几个可选参数，所以我们需要显式地指定掩码参数，就像这样:`mask = mask_image`。结果是图像只显示了我们要过滤的颜色。

演示这一点的最简单的方法是通过它走一走。一旦你知道发生了什么，代码其实很简单。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`blue_filter.py`。
3.  输入以下代码:

    ```py
    import cv2

    img = cv2.imread("color_balls_small.jpg")
    imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

    lower_blue = np.array([80,120,120])
    upper_blue = np.array([130,255,255])

    blueMask = cv2.inRange(imgHSV,lower_blue,upper_blue)

    res = cv2.bitwise_and(img, img, mask=blueMask)

    cv2.imshow('img', img)
    cv2.imshow('mask', blueMask)
    cv2.imshow('blue', res)

    cv2.waitKey(0)

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`python blue_filter.py`并按回车键。

打开三个窗口，显示我们图像的不同版本。首先是常规图像。第二张是黑白图像，充当我们的面具。第三个是最终的蒙版图像。仅显示蒙版白色区域下的像素。

让我们花点时间浏览一下代码，弄清楚我们在做什么以及为什么做。

我们像处理所有脚本一样开始，先导入 OpenCV 和 NumPy，然后加载图像。

```py
import cv2
import numpy as np
img = cv2.imread("color_balls_small.jpg")

```

接下来，我们复制图像并将其转换到 HSV 颜色空间。

```py
imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

```

一旦进入 HSV 颜色空间，就更容易过滤蓝色球。正如我所讨论的，我们知道纯蓝色的色调值为 120。因为我们过滤的对象不太可能是纯蓝色的，所以我们需要给它一个颜色范围。在这种情况下，我们要寻找从 80(介于绿色和蓝色之间)到 130 的所有内容。我们还想过滤不是接近白色或接近黑色的颜色，所以我们使用值 120 和 255 作为饱和度和值范围。为了确保过滤器范围是 OpenCV 能够理解的格式，我们将它们创建为 NumPy 数组。

```py
lower_blue = np.array([80,120,120])
upper_blue = np.array([130,255,255])

```

有了指定的滤镜范围，我们可以使用它们和`inRange()`方法来确定图像的 HSV 版本中的像素是否在我们正在寻找的蓝色范围内。这将创建遮罩图像以排除所有非蓝色像素。

```py
blueMask = cv2.inRange(imgHSV,lower_blue,upper_blue)

```

接下来，我们使用`bitwise_and()`来应用我们的面具。因为我们希望返回蒙版中图像的所有像素，所以我们将原始图像作为图像 1 和图像 2 传递。这将图像与其自身进行比较，并返回整个图像，因为图像中的每个像素都与其自身匹配。

```py
res = cv2.bitwise_and(img, img, mask=blueMask)

```

最后，我们显示原始图像、遮罩和过滤后的图像。然后，在我们关闭窗口并退出程序之前，我们等待一个键被按下。

```py
cv2.imshow('img', img)
cv2.imshow('mask', blueMask)
cv2.imshow('blue', res)

cv2.waitKey(0)

```

正如你所看到的，一旦你知道它是如何工作的，过滤颜色是非常容易的。当你过滤红色时，事情变得有点复杂。红色出现在色调光谱的低端和高端，因此您必须创建两个滤镜并组合生成的遮罩。这可以很容易地用 OpenCV 的`add()`方法来完成，它看起来像这样:

```py
combinedMask = cv2.add(redMask1, redMask2)

```

最后，你得到的图像只有你想要的像素。对人眼来说，它很容易被识别为相关组。对于计算机来说，情况并非如此。本来，计算机不能识别黑色像素和蓝色像素之间的差异。这就是斑点检测发挥作用的地方。

### 斑点和斑点检测

斑点是相似像素的集合。它们可以是任何东西，从单调的圆形到 jpeg 图像。对计算机来说，像素就是像素，它无法区分球的图像和平面的图像。这就是计算机视觉如此具有挑战性的原因。我们已经开发了许多不同的技术来尝试推断关于图像的信息；每一种都在速度和准确性方面有所取舍。

大多数技术使用称为特征提取的过程，特征提取是一组算法的总称，这些算法对图像中的突出特征进行分类，如线条、边缘、大范围颜色等。一旦提取了这些特征，就可以对它们进行分析或与其他特征进行比较，以确定图像。这就是面部检测和运动检测等功能的工作原理。

我们将使用一种更简单的方法来跟踪一个对象。我们将使用上一节中的颜色过滤技术来识别大面积的颜色，而不是提取细节特征并进行分析。然后，我们将使用内置函数来收集关于像素组的信息。这种更简单的技术被称为斑点检测。

#### 找到一个斑点

OpenCV 使得斑点检测变得相当容易，尤其是在我们过滤掉所有我们不想要的东西之后。一旦图像被过滤，我们可以使用掩模进行干净斑点检测。OpenCV 的`SimpleBlobDetector`类识别斑点的位置和大小。

这个类并不像你想象的那么简单。它内置了许多需要启用或禁用的参数。如果启用，您需要确保这些值适用于您的应用。

设置参数的方法是`SimpleBlobDetector_Params()`。创建检测器的方法是`SimpleBlobDetector_create()`。您将参数传递给 create 方法，以确保一切设置正确。

一旦设置好参数并正确创建了检测器，就可以使用`detect()`方法来识别关键点。在简单斑点检测器的情况下，关键点表示任何检测到的斑点的中心和大小。

最后，我们使用`drawKeyPoints()`方法在斑点周围画一个圆。默认情况下，这会在斑点的中心绘制一个小圆。然而，可以传递一个标志，使得圆的大小相对于斑点的大小。

让我们看一个例子。我们将使用之前练习中的过滤器代码，并添加斑点检测。在本练习中，我们过滤图像中的蓝色球。然后我们用蒙版找到球的中心，围绕它画一个圆。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`simple_blob_detect.py`。
3.  输入以下代码:

    ```py
    import cv2
    import numpy as np

    img = cv2.imread("color_balls_small.jpg")
    imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

    # setup parameters
    params = cv2.SimpleBlobDetector_Params()

    params.filterByColor = False
    params.filterByArea = False
    params.filterByInertia = False
    params.filterByConvexity = False
    params.filterByCircularity = False

    # create blob detector
    det = cv2.SimpleBlobDetector_create(params)

    lower_blue = np.array([80,120,120])
    upper_blue = np.array([130,255,255])

    blueMask = cv2.inRange(imgHSV,lower_blue,upper_blue)

    res = cv2.bitwise_and(img, img, mask=blueMask)

    # get keypoints
    keypnts = det.detect(blueMask)

    # draw keypoints
    cv2.drawKeypoints(img, keypnts, img, (0,0,255),
           cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

    cv2.imshow('img', img)
    cv2.imshow('mask', blueMask)
    cv2.imshow('blue', res)

    # print the coordinates and size of keypoints to terminal
    for k in keypnts:
        print k.pt[0]
        print k.pt[1]
        print k.size

    cv2.waitKey(0)

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`python simple_blob_detect.py`并按回车键。

这将打开图像的三个版本。但是，原始图像现在有一个红色的圆圈围绕着蓝色的球。在终端窗口中，我们打印了球的中心坐标和大小。在本章的后面，当我们开始跟踪球时，会用到球的中心。

##### 参数

`SimpleBlobDetector`类需要几个参数才能正常工作。强烈建议通过将相应的参数设置为 True 或 False 来显式启用或禁用所有过滤器选项。如果启用了过滤器，您还需要为其设置参数。默认参数被配置为提取黑色圆形斑点。

在上一个练习中，我们简单地禁用了所有过滤器。由于我们正在处理一个球的过滤图像，并且我们在图像中只有一个斑点，我们不需要添加其他过滤器。虽然从技术上讲，您可以单独使用 SimpleBlobDetector 的参数，而不屏蔽所有其他的参数，但是在拨入所有参数以获得我们想要的结果时，这可能会更具挑战性。此外，我们使用的方法允许您更深入地了解 OpenCV 在后台做什么。

理解 SimpleBlobDetector 的工作方式对于更好地理解如何使用过滤器是很重要的。有几个参数可用于微调结果。

首先发生的是通过应用阈值将图像转换成几个二值图像。`minThreshold`和`maxThreshold`决定整体范围，而`thresholdStep`决定阈值之间的距离。

然后使用`findContours()`对每个二进制图像进行轮廓处理。这允许系统计算每个斑点的中心。已知中心后，使用`minDistanceBetweenBlobs`参数将几个斑点组合成一组。

组的中心作为关键点返回，组的总直径也是如此。计算每个滤波器的参数并应用滤波器。

##### 过滤器

下面列出了过滤器及其相应的参数。

###### filterByColor

这将过滤每个二进制图像的相对强度。它测量斑点中心的强度值，并将其与参数`blobColor`进行比较。如果它们不匹配，则该斑点不合格。强度从 0 到 255 测量；0 为暗，255 为亮。

###### 过滤区域

当单个斑点被分组时，计算它们的总面积。该过滤器寻找`minArea`和`maxArea`之间的斑点。

###### 过滤循环

圆度通过公式

![$$ \frac{4^{\ast }{\pi}^{\ast } Area}{perimeter^{\ast } perimeter} $$](A457480_1_En_9_Chapter_Equa.gif)

计算

这将返回一个介于 0 和 1 之间的比率，该比率将与`minCircularity`和`maxCircularity`进行比较。如果该值在这些参数之间，则结果中包含该斑点。

###### 过滤惯性

惯性是对斑点被拉长程度的估计。它是一个介于 0 和 1 之间的比率。如果值在`minInertiaRatio`和`maxInertiaRatio`之间，则在关键点结果中返回斑点。

###### filterby 凸性

凸度是一个介于 0 和 1 之间的比值。它测量斑点中凸曲线和凹曲线之间的比率。凸度参数为`minConvexity`和`maxConvexity`。

#### 斑点跟踪

我们在上一节中看到，斑点中心的 x 和 y 坐标作为关键点的一部分返回，用于跟踪斑点。要跟踪 blob，您需要使用来自机器人摄像机的实时视频流，然后定义跟踪对您的项目意味着什么。最简单的跟踪形式是简单地用斑点移动生成的圆。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`blob_tracker.py`。
3.  输入以下代码:

    ```py
    import cv2
    import numpy as np

    cap = cv2.VideoCapture(0)

    # setup detector and parameters
    params = cv2.SimpleBlobDetector_Params()

    params.filterByColor = False
    params.filterByArea = True
    params.minArea = 20000
    params.maxArea = 30000
    params.filterByInertia = False
    params.filterByConvexity = False
    params.filterByCircularity = True
    params.minCircularity = 0.5
    params.maxCircularity = 1

    det = cv2.SimpleBlobDetector_create(params)

    # define blue
    lower_blue = np.array([80,60,20])
    upper_blue = np.array([130,255,255])

    while True:
        ret, frame = cap.read()

        imgHSV = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

        blueMask = cv2.inRange(imgHSV,lower_blue,upper_blue)
        blur= cv2.blur(blueMask, (10,10))

        res = cv2.bitwise_and(frame, frame, mask=blueMask)

        # get and draw keypoint
        keypnts = det.detect(blur)

        cv2.drawKeypoints(frame, keypnts, frame, (0,0,255),
                          cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

        cv2.imshow('frame', frame)
        cv2.imshow('mask', blur)

        for k in keypnts:
            print k.size

        if cv2.waitKey(1) & 0xff == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`sudo python blob_tracker.py`并按回车键。

两个窗口打开:一个显示用于过滤颜色的遮罩，另一个显示视频流。应该在斑点周围画一个圆。

我启用了`filterByArea`和`filterByCircularity`来确保我只得到球。您可能需要调整检测器的参数来微调过滤器。

## 追球机器人

你现在知道如何用安装在机器人上的网络摄像头跟踪一个斑点了。在第 8 章中，你学习了一种叫做 PID 控制器的算法。当我们将 PID 控制器与我们的球跟踪程序结合起来时会发生什么？

接下来，让我们给这个小机器人编程，让它去追逐它一直在追踪的那个蓝色的球。为此，您将使用您刚刚学到的关于斑点跟踪的知识以及您在第 [8](08.html) 章中学到的知识。

PID 控制器期望以偏离期望结果的形式输入。因此，我们需要从定义期望的结果开始。在这种情况下，目标只是将球保持在框架的中间。因此，我们的误差值将是中心的方差，这也意味着我们需要定义帧的中心。一旦我们定义了中心，偏差就是从中心的 x 位置减去球的 x 位置。我们也将从中心的 y 位置减去球的 y 位置。

现在我们可以使用两个 PID 控制器来保持球在框架的中心。第一个控制器操纵机器人。当球在 x 轴上运动时，偏差要么是负的，要么是正的。如果是积极的，转向左边。如果是负数，转向右边。同样，我们可以用 y 轴来控制机器人的速度。正 y 方差驱动机器人前进，而负 y 方差驱动机器人后退。

1.  打开空闲的 IDE 并创建一个新文件。
2.  将文件另存为`ball_chaser.py`。
3.  输入以下代码:

    ```py
    import cv2
    import numpy as np
    import time

    from Adafruit_MotorHAT import Adafruit_MotorHAT as amhat
    from Adafruit_MotorHAT import Adafruit_DCMotor as adamo

    # create motor objects
    motHAT = amhat(addr=0x60)
    mot1 = motHAT.getMotor(1)
    mot2 = motHAT.getMotor(2)
    mot3 = motHAT.getMotor(3)
    mot4 = motHAT.getMotor(4)

    motors = [mot1, mot2, mot3, mot4]

    # motor multipliers
    motorMultiplier = [1.0, 1.0, 1.0, 1.0, 1.0]

    # motor speeds
    motorSpeed = [0,0,0,0]

    # speeds
    speedDef = 100
    leftSpeed = speedDef
    rightSpeed = speedDef
    diff= 0
    maxDiff = 50
    turnTime = 0.5

    # create camera object
    cap = cv2.VideoCapture(0)
    time.sleep(1)

    # PID
    kp = 1.0
    ki = 1.0
    kd = 1.0
    ballX = 0.0
    ballY = 0.0

    x = {'axis':'X',
         'lastTime':int(round(time.time()*1000)),
         'lastError':0.0,
         'error':0.0,
         'duration':0.0,
         'sumError':0.0,
         'dError':0.0,
         'PID':0.0}
    y = {'axis':'Y',
         'lastTime':int(round(time.time()*1000)),
         'lastError':0.0,
         'error':0.0,
         'duration':0.0,
         'sumError':0.0,
         'dError':0.0,
         'PID':0.0}

    # setup detector
    params = cv2.SimpleBlobDetector_Params()

    # define detector parameters
    params.filterByColor = False
    params.filterByArea = True
    params.minArea = 15000
    params.maxArea = 40000
    params.filterByInertia = False
    params.filterByConvexity = False
    params.filterByCircularity = True
    params.minCircularity = 0.5
    params.maxCircularity = 1

    # create blob detector object
    det = cv2.SimpleBlobDetector_create(params)

    # define blue
    lower_blue = np.array([80,60,20])
    upper_blue = np.array([130,255,255])

    def driveMotors(leftChnl = speedDef, rightChnl = speedDef,
                    duration = defTime):
        # determine the speed of each motor by multiplying
        # the channel by the motors multiplier
        motorSpeed[0] = leftChnl * motorMultiplier[0]
        motorSpeed[1] = leftChnl * motorMultiplier[1]
        motorSpeed[2] = rightChnl * motorMultiplier[2]
        motorSpeed[3] = rightChnl * motorMultiplier[3]

        # set each motor speed. Since the speed can be a
        # negative number, we take the absolute value
        for x in range(4):
            motors[x].setSpeed(abs(int(motorSpeed[x])))

        # run the motors. if the channel is negative, run
        # reverse. else run forward
        if(leftChnl < 0):
            motors[0].run(amhat.BACKWARD)
            motors[1].run(amhat.BACKWARD)
        else:
            motors[0].run(amhat.FORWARD)
            motors[1].run(amhat.FORWARD)

        if (rightChnl > 0):
            motors[2].run(amhat.BACKWARD)
            motors[3].run(amhat.BACKWARD)
        else:
            motors[2].run(amhat.FORWARD)
            motors[3].run(amhat.FORWARD)

    def PID(axis):
        lastTime = axis['lastTime']
        lastError = axis['lastError']

        # get the current time
        now = int(round(time.time()*1000))
        duration = now-lastTime

        # calculate the error
        axis['sumError'] += axis['error'] * duration
        axis['dError'] = (axis['error'] - lastError)/duration

        # prevent runaway values
        if axis['sumError'] > 1:axis['sumError'] = 1
        if axis['sumError'] < -1: axis['sumError'] = -1

        # calculate PID
        axis['PID'] = kp * axis['error'] + ki * axis['sumError'] + kd * axis['dError']

        # update variables
        axis['lastError'] = axis['error']
        axis['lastTime'] = now

        # return the output value
        return axis

    def killMotors():
        mot1.run(amhat.RELEASE)
        mot2.run(amhat.RELEASE)
        mot3.run(amhat.RELEASE)
        mot4.run(amhat.RELEASE)

    # main program
    try:
        while True:
            # capture video frame
            ret, frame = cap.read()

            # calculate center of frame
            height, width, chan = np.shape(frame)
            xMid = width/2 * 1.0
            yMid = height/2 * 1.0

            # filter image for blue ball
            imgHSV = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

            blueMask = cv2.inRange(imgHSV, lower_blue, upper_blue)
            blur = cv2.blur(blueMask, (10,10))

            res = cv2.bitwise_and(frame,frame,mask=blur)

            # get keypoints
            keypoints = det.detect(blur)
            try:
                ballX = int(keypoints[0].pt[0])
                ballY = int(keypoints[0].pt[1])
            except:
                pass

            # draw keypoints
            cv2.drawKeypoints(frame, keypoints, frame, (0,0,255),
                              cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

            # calculate error and get PID ratio
            xVariance = (ballX - xMid) / xMid
            yVariance = (yMid - ballY) / yMid

            x['error'] = xVariance/xMid
            y['error'] = yVariance/yMid

            x = PID(x)
            y = PID(y)

            # calculate left and right speeds
            leftSpeed = (speedDef * y['PID']) + (maxDiff * x['PID'])
            rightSpeed = (speedDef * y['PID']) - (maxDiff * x['PID'])

            # another safety check for runaway values
            if leftSpeed > (speedDef + maxDiff): leftSpeed = (speedDef + maxDiff)
            if leftSpeed < -(speedDef + maxDiff): leftSpeed = -(speedDef + maxDiff)
            if rightSpeed > (speedDef + maxDiff): rightSpeed = (speedDef + maxDiff)
            if rightSpeed < -(speedDef + maxDiff): rightSpeed = -(speedDef + maxDiff)

            # drive motors
            driveMotors(leftSpeed, rightSpeed, driveTime)

            # show frame
    ##        cv2.imshow('frame', frame)
    ##        cv2.waitKey(1)

    except KeyboardInterrupt:
        killMotors()
        cap.release()
        cv2.destroyAllWindows()

    ```

4.  保存文件。
5.  打开终端窗口。
6.  导航到保存文件的文件夹。
7.  输入`sudo python ball_chaser.py`并按回车键。

几秒钟后，你的机器人应该开始向前移动。如果框架内有一个蓝色的球，它应该转向它。机器人试图将球保持在框架的中心。

这段代码中的一些内容与我们过去的做法有些不同。最值得注意的是，我们将 x 和 y 轴的值放入字典中。我们这样做是为了在将值传递给 PID 控制器时将它们保持在一起，这是所做的另一项更改。PID 函数已更新为接受单个参数。然而，它所期望的参数是一个字典。它被赋给函数中的轴变量。然后所有的变量引用都被更新以使用字典。结果在 axis 字典中更新，然后分配给主程序中的适当字典。

我还确保消除任何会影响主循环或相机刷新率的延迟。因为整个程序是在单个进程中运行的，所以它没有我们将进程分解成不同线程时那么快。因此，机器人可能会错过球并跑偏。

## 摘要

在这一章中，我们开始利用 Raspberry Pi 提供的一些令人兴奋的功能。与单独使用微控制器相比，计算机视觉允许我们执行更复杂的任务。

为了准备使用视觉系统，我们在机器人上安装了一个基本的网络摄像头。这需要特别考虑，因为这些网络摄像头不是为安装而设计的。当然，你的解决方案可能与我的不同，所以你可以在安装相机时发挥一些创造力。之后，我们准备安装 OpenCV。

OpenCV 是一个开源社区开发的计算机视觉平台，它使许多视觉功能变得非常简单。在 Raspberry Pi 上安装软件需要相当长的时间，主要是因为我们必须从源代码编译它，尽管它的功能令人印象深刻，但 Raspberry Pi 没有笔记本电脑或 PC 的处理能力，所以编译代码需要一段时间。但是一旦编译和安装，我们就能做一些有趣的事情。

我们用静止图像做了一些练习。这让我们可以学习 OpenCV 的一些基础知识，而不需要处理视频。一旦我们学会了一些基础知识，我们就学会了从摄像机中提取现场视频，并运用我们在静态图像中学到的知识。使用我们在这一章学到的颜色过滤和斑点追踪技术，我们给了我们的机器人看见和跟随一个球的能力。