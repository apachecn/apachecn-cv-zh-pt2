# 监督学习的第一步

这是你一直在等待的时刻，不是吗？

我们已经覆盖了所有的基础——我们有一个运行良好的 Python 环境，我们安装了 OpenCV，我们知道如何用 Python 处理数据。现在，是时候构建我们的第一个机器学习系统了！还有什么比专注于最常见和最成功的机器学习类型之一:**监督学习**更好的开始方式呢？

从上一章中，我们已经知道监督学习是通过使用附带的标签来学习训练数据中的规则，以便我们可以预测一些新的、从未见过的测试数据的标签。在这一章中，我们想深入一点，学习如何将我们的理论知识...

# 技术要求

可以通过以下链接查阅本章代码:[https://github . com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/chapter 03](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter03)。

以下是软件和硬件要求的全球总结:

*   您将需要 OpenCV 版本 4.1.x (4.1.0 或 4.1.1 都可以)。
*   您将需要 Python 3.x 版本(任何 Python 3.x 版本都可以)。
*   您将需要 Anaconda Python 3 来安装 Python 和所需的模块。
*   这本书可以使用任何操作系统——苹果操作系统、视窗操作系统和基于 Linux 的操作系统。我们建议您的系统中至少有 4 GB 内存。
*   运行本书提供的代码不需要 GPU。

# 理解监督学习

我们之前已经确定，监督学习的目标总是预测数据的标签(或目标值)。然而，根据这些标签的性质，监督学习可以有两种不同的形式:

*   **分类**:每当我们使用数据预测类别时，监督学习就被称为**分类**。这方面的一个很好的例子是，当我们试图预测一个图像是否包含一只猫或一只狗。在这里，数据的标签是分类的，不是一个就是另一个，但绝不是类别的混合。例如，一张图片包含一只猫或一只狗，从不包含 50%的猫和 50%的狗(在你问之前，不，这里我们不考虑卡通人物 CatDog 的图片)，以及我们的工作...

# 看看 OpenCV 中的监督学习

如果我们不能将其付诸实践，仅仅知道监督学习是如何工作的是没有任何用处的。值得庆幸的是，OpenCV 为其所有的统计学习模型提供了一个相当简单的界面，其中包括所有的监督学习模型。

在 OpenCV 中，每一个机器学习模型都源自`cv::ml::StatModel`基类。这是一个花哨的说法，说如果我们想在 OpenCV 中使用机器学习模型，我们必须提供`StatModel`告诉我们的所有功能。这包括训练模型的方法(称为`train`)和测量模型性能的方法(称为`calcError`)。

In **Object-Oriented Programming** (**OOP**), we deal primarily with objects or classes. An object consists of several functions, called **methods**, as well as variables, called **members** or **attributes**. You can learn more about OOP in Python at [https://docs.python.org/3/tutorial/classes.html](https://docs.python.org/3/tutorial/classes.html).

由于软件的这种组织，在 OpenCV 中建立机器学习模型总是遵循相同的逻辑，我们将在后面看到:

*   **初始化**:我们通过名称来调用模型，创建一个模型的空实例。
*   **设置参数**:如果模型需要一些参数，我们可以通过 setter 方法进行设置，每个模型可以不同。例如，要使 k-NN 算法工作，我们需要指定其开放参数 *k* (我们将在后面找到)。
*   **训练模型**:每个模型都必须提供一个叫做`train`的方法，用来将模型拟合到一些数据。
*   **预测新标签**:每个模型都必须提供一个叫做`predict`的方法，用来预测新数据的标签。
*   **给模型打分**:每个模型都必须提供一个叫做`calcError`的方法，用来衡量性能。这种计算可能对每个模型都不同。

Because OpenCV is a vast and community-driven project, not every algorithm follows these rules to the extent that we as users might expect. For example, the k-NN algorithm does most of its work in a `findNearest` method, although `predict` still works. We will make sure to point out these discrepancies as we work through different examples.

由于我们会偶尔使用 scikit-learn 来实现一些 OpenCV 没有提供的机器学习算法，因此值得指出的是，scikit-learn 中的学习算法遵循几乎相同的逻辑。最显著的区别是 scikit-learn 在初始化步骤中设置了所有必需的模型参数。另外，它调用训练函数`fit`，而不是`train`，调用评分函数`score`，而不是`calcError`。

# 用评分函数测量模型性能

构建机器学习系统最重要的部分之一是找到一种方法来衡量模型预测的质量。在现实生活中，一个模型很少会把所有事情都做好。从前面的章节中，我们知道我们应该使用测试集中的数据来评估我们的模型。但是这到底是怎么回事呢？

简短但不太有用的答案是，这取决于模型。人们提出了各种各样的评分函数，可以用来评估所有可能场景中的训练模型。好消息是，它们中的许多实际上是 scikit-learn 的`metrics`模块的一部分。

让我们快速了解一些最重要的评分功能。...

# 使用准确度、精确度和召回率对分类器进行评分

在只有两个不同类别标签的二进制分类任务中，有几种不同的方法来衡量分类性能。一些常见的指标如下:

*   `accuracy_score`:准确度计算测试集中已被正确预测的数据点的数量，并将该数量作为测试集大小的一部分返回。坚持图片分类为猫或狗的例子，准确性表示图片被正确分类为包含猫或狗的部分。这是分类器最基本的评分功能。
*   `precision_score` : Precision 描述分类器不将包含狗的图片标记为猫的能力。换句话说，在测试集中分类器认为包含猫的所有图片中，精度是实际包含猫的图片的分数。
*   `recall_score`:回忆(或敏感度)描述分类器检索包含猫的所有图片的能力。换句话说，在测试集中的所有猫的图片中，回忆是被正确识别为猫的图片的比例。

假设我们有一些`ground truth`(根据我们拥有的数据集正确)类标签不是 0 就是 1。我们可以使用 NumPy 的随机数生成器随机生成它们。显然，这意味着，每当我们重新运行代码时，都会随机生成新的数据点。然而，对于这本书的目的来说，这不是很有帮助，因为我希望您能够运行代码，并总是得到与我相同的结果。一个很好的技巧是固定随机数发生器的种子。这将确保生成器在每次运行脚本时以相同的方式初始化:

1.  我们可以使用以下代码修复随机数生成器的种子:

```py
In [1]: import numpy as np
In [2]: np.random.seed(42)
```

2.  然后，我们可以通过在范围`(0,2)`中选择随机整数来生成五个随机标签，它们要么是 0，要么是 1:

```py
In [3]: y_true = np.random.randint(0, 2, size=5)
...     y_true
Out[3]: array([0, 1, 0, 0, 0])
```

In the literature, these two classes are sometimes also called **positives** (all data points with the class label, `1`) and **negatives** (all other data points).

让我们假设我们有一个分类器，试图预测前面提到的类标签。为了论证，假设分类器不是很聪明，总是预测标签，`1`。我们可以通过硬编码预测标签来模拟这种行为:

```py
In [4]: y_pred = np.ones(5, dtype=np.int32)
...     y_pred
Out[4]: array([1, 1, 1, 1, 1], dtype=int32)
```

我们预测的准确性如何？

如前所述，准确性计算测试集中已被正确预测的数据点的数量，并将该数量作为测试集大小的一部分返回。我们只正确预测了第二个数据点(这里真正的标签是`1`)。在所有其他情况下，真正的标签是`0`，然而我们预测`1`。因此，我们的精度应该是 1/5 或 0.2。

准确性度量的简单实现可能会汇总预测的类标签与真实的类标签匹配的所有情况:

```py
In [5]: test_set_size = len(y_true)
In [6]: predict_correct = np.sum(y_true == y_pred)
In [7]: predict_correct / test_set_size
Out[7]: 0.2
```

scikit-learn 的`metrics`模块提供了更智能、更方便的实现:

```py
In [8]: from sklearn import metrics
In [9]: metrics.accuracy_score(y_true, y_pred)
Out[9]: 0.2
```

这并不太难，是吗？然而，要理解精度和召回率，我们需要对ⅰ型和ⅱ型错误有一个大致的了解。我们回想一下，带有类标签`1`的数据点通常被称为阳性，带有类标签`0`(或-1)的数据点通常被称为阴性。然后，对特定数据点进行分类可以有四种可能的结果之一，如下面的混淆矩阵所示:

|  | **是真正的正** | **是真正的负** |
| **预测阳性** | 正确肯定 | 假阳性 |
| **预测负值** | 假阴性 | 正确否定 |

让我们把它分解一下。如果一个数据点真的是正的，并且我们预测是正的，那么我们就做对了！在这种情况下，结果被称为**真阳性**。如果我们认为数据点是正的，但实际上是负的，我们就错误地预测了正的(因此有了这个术语，**假阳性**)。类似地，如果我们认为数据点是负的，但它确实是正的，我们就错误地预测了负的(假负的)。最后，如果我们预测了一个负数，而数据点确实是负数，我们就找到了一个真正的负数。

In statistical hypothesis testing, false positives are also known as **type I errors** and false negatives are also known as **type II errors**.

让我们根据模型数据快速计算这四个指标。我们有一个真正的正，其中真正的标签是`1`，我们预测`1`:

```py
In [10]: truly_a_positive = (y_true == 1)
In [11]: predicted_a_positive = (y_pred == 1)
In [12]: true_positive = np.sum(predicted_a_positive * truly_a_positive )
...      true_positive
Out[12]: 1
```

同样，假阳性是我们预测的`1`，但`ground truth`实际上是`0`:

```py
In [13]: false_positive = np.sum((y_pred == 1) * (y_true == 0))
...      false_positive
Out[13]: 4
```

我相信现在你已经掌握了窍门。但是我们需要做数学才能知道预测的否定吗？我们不太聪明的分类器从来没有预测过`0`，所以`(y_pred == 0)`永远不会是真的:

```py
In [14]: false_negative = np.sum((y_pred == 0) * (y_true == 1))
...      false_negative
Out[14]: 0
In [15]: true_negative = np.sum((y_pred == 0) * (y_true == 0))
...      true_negative
Out[15]: 0
```

让我们也画出混淆矩阵:

|  | **是真正的正** | **是真正的负** |
| **预测阳性** | one | four |
| **预测负值** | Zero | Zero |

为了确保我们做对了所有的事情，让我们再计算一次精确度。准确性应该是真阳性的数量加上真阴性的数量(也就是我们得到的所有正确的东西)除以数据点的总数:

```py
In [16]: accuracy = (true_positive + true_negative) / test_set_size
...      accuracy
Out[16]: 0.2
```

成功！然后给出精度，即真阳性数除以所有真预测数:

```py
In [17]: precision = true_positive / (true_positive + false_positive)
...      precision
Out[17]: 0.2
```

事实证明，在我们的情况下，精确度并不比准确度好。让我们用 scikit-learn 来检查我们的数学:

```py
In [18]: metrics.precision_score(y_true, y_pred)
Out[18]: 0.2
```

最后，`recall`作为我们正确分类为阳性的所有阳性的分数给出:

```py
In [19]: recall = true_positive / (true_positive + false_negative)
...      recall
Out[19]: 1.0
In [20]: metrics.recall_score(y_true, y_pred)
Out[20]: 1.0
```

完美回忆！但是，回到我们的模型数据，应该很清楚，这个出色的回忆得分只是运气。由于在我们的模型数据集中只有一个`1`标签，并且我们碰巧正确地对它进行了分类，所以我们得到了一个完美的召回分数。这是否意味着我们的分类器是完美的？不是真的！但是，我们发现了三个有用的指标，似乎可以衡量我们分类性能的互补方面。

# 使用均方误差、解释方差和 R 平方对回归进行评分

就回归模型而言，我们的指标，如前所示，不再起作用了。毕竟，我们现在预测的是连续的输出值，而不是截然不同的分类标签。幸运的是，scikit-learn 提供了一些其他有用的评分功能:

*   `mean_squared_error`:回归问题最常用的误差度量是测量训练集中每个数据点的预测值和真实目标值之间的平方误差，所有数据点的平均值。
*   `explained_variance_score`:一个更复杂的度量是衡量一个模型能在多大程度上解释测试数据的变化或分散。通常，解释的数量...

# 使用分类模型预测类别标签

有了这些工具，我们现在可以进行第一个真正的分类示例。

以兰多姆维尔小镇为例，那里的人们对他们的两支运动队——兰多姆维尔红军和兰多姆维尔蓝军——非常着迷。红军已经存在很长时间了，人们喜欢他们。但是后来，一些外地的百万富翁出现了，买下了红军的最佳射手，并组建了一支新的球队，蓝军。令大多数红魔球迷不满的是，这位最佳射手将随蓝军赢得冠军。几年后，他将回到红军，尽管球迷们对他之前的职业选择有些反感，但他们永远不会原谅他。但无论如何，你可以看到为什么红魔的球迷不一定能和蓝军的球迷相处融洽。事实上，这两个粉丝群是如此的分裂，以至于他们甚至从来没有住在一起。我甚至听过这样的故事:一旦蓝军球迷搬进隔壁，红球迷就故意搬走。真实故事！

不管怎样，我们是新来的，正试着挨家挨户地向人们推销一些蓝调商品。然而，我们时不时会遇到一个心碎的红军球迷，他会因为我们卖蓝军的东西而对我们大喊大叫，并把我们赶出他们的草坪。不好听！完全避开这些房子，转而去拜访蓝军球迷，压力会小得多，也能更好地利用我们的时间。

自信我们可以学会预测红军球迷住在哪里，我们开始记录我们的遭遇。如果我们路过一个红军球迷的家，我们会在手边的城镇地图上画一个红色三角形；否则，我们画一个蓝色的正方形。过了一会儿，我们对每个人的居住地有了一个很好的了解:

![](Images/ced3ad58-fea3-4305-8752-ecebfde7cc9f.png)

然而，现在，我们接近了前面地图中标记为绿色圆圈的房子。我们应该敲他们的门吗？我们试图找到一些他们更喜欢哪支球队的线索(也许是挂在后门廊的球队旗帜)，但我们看不到任何线索。我们怎么知道敲他们的门是否安全？

这个愚蠢的例子说明的正是监督学习算法可以解决的那种问题。我们有一堆观察数据(房子、它们的位置和颜色)组成了我们的训练数据。我们可以利用这些数据从经验中学习，这样，当我们面临预测新房子颜色的任务时，我们就可以做出明智的估计。

正如我们前面提到的，红魔的球迷对他们的球队真的很有激情，所以他们永远不会靠近蓝军球迷。难道我们不能利用这些信息，看看所有邻近的房子，找出什么样的风扇住在新房子里？

这正是 k-NN 算法要做的。

# 理解 k-NN 算法

k-NN 算法可以说是最简单的机器学习算法之一。这样做的原因是我们基本上只需要存储训练数据集。然后，为了预测一个新的数据点，我们只需要在训练数据集中找到最近的数据点:它的最近邻居。

简而言之，k-NN 算法认为一个数据点可能与其邻居属于同一类。想想看:如果我们的邻居是红魔球迷，我们可能也是红魔球迷；否则，我们早就搬走了。蓝军也是如此。

当然，有些街区可能更复杂一点。在这种情况下，我们不仅要考虑我们最近的邻居(其中 *k=1* ，而是...

# 在 OpenCV 中实现 k-NN

使用 OpenCV，我们可以通过`cv2.ml.KNearest_create()`函数轻松创建 k-NN 模型。然后，构建模型包括以下步骤:

1.  生成一些训练数据。
2.  为给定的数字 *k* 创建一个 k-NN 对象。

3.  找到我们要分类的新数据点的 *k* 最近邻。
4.  通过多数票分配新数据点的类别标签。
5.  画出结果。

我们首先导入所有必要的模块:用于 k-NN 算法的 OpenCV、用于数据处理的 NumPy 和用于绘图的 Matplotlib。如果你在 Jupyter 笔记本上工作，别忘了召唤`%matplotlib inline`魔法:

```py
In [1]: import numpy as np
...     import cv2
...     import matplotlib.pyplot as plt
...     %matplotlib inline
In [2]: plt.style.use('ggplot')
```

# 生成训练数据

第一步是生成一些训练数据。为此，我们将使用 NumPy 的随机数生成器。如前一节所述，我们将修复随机数生成器的种子，这样重新运行脚本将始终生成相同的值:

```py
In [3]: np.random.seed(42)
```

好了，现在我们开始吧。我们的训练数据到底应该是什么样的？

在前面的示例中，每个数据点都是城镇地图上的一所房子。每个数据点都有两个特征(即其在城镇地图上的位置的 *x* 和 *y* 坐标)和一个类别标签(即，如果蓝调粉丝住在那里，则为蓝色正方形，如果红调粉丝住在那里，则为红色三角形)。

因此，可以表示单个数据点的特征...

# 训练分类器

与所有其他机器学习功能一样，k-NN 分类器是 OpenCV 3.1 `ml`模块的一部分。我们可以使用以下命令创建一个新的分类器:

```py
In [15]: knn = cv2.ml.KNearest_create()
```

In older versions of OpenCV, this function might be called `cv2.KNearest()` instead.

然后，我们将训练数据传递给`train`方法:

```py
In [16]: knn.train(train_data, cv2.ml.ROW_SAMPLE, labels)
Out[16]: True
```

在这里，我们必须告诉`knn`我们的数据是一个*N×2*数组(也就是说，每一行都是一个数据点)。成功后，功能返回`True`。

# 预测新数据点的标签

`knn`提供的另一个真正有用的方法叫做`findNearest`。它可用于根据最近邻预测新数据点的标签。

得益于我们的`generate_data`功能，生成一个新的数据点其实真的很容易！我们可以把一个新的数据点想象成一个大小为`1`的数据集:

```py
In [17]: newcomer, _ = generate_data(1)...      newcomerOut[17]: array([[91., 59.]], dtype=float32)
```

我们的函数也返回一个随机标签，但是我们对此不感兴趣。相反，我们想用我们训练好的分类器来预测它！我们可以告诉 Python 忽略带有下划线(`_`)的输出值。

让我们再看看我们的城镇地图。我们将像前面一样绘制训练集，但是...

# 使用回归模型预测连续结果

现在，让我们把注意力转向一个回归问题。我相信你现在可以在睡梦中背诵，回归是关于预测连续的结果，而不是预测离散的类别标签。

# 理解线性回归

最简单的回归模型叫做**线性回归**。线性回归背后的思想是用特征的线性组合来描述一个目标变量(比如波士顿房价——回想一下我们在[第 1 章](01.html)、*机器学习的味道*中研究的各种数据集)。

为了简单起见，我们只关注两个特性。假设我们想使用两个特征来预测明天的股价:今天的股价和昨天的股价。我们将今天的股价表示为第一特征， *f <sub>1</sub>* ，昨天的股价表示为 *f <sub>2</sub>* 。那么，线性回归的目标就是学习两个权重系数， *w <sub>1</sub>* 和 *w <sub>2</sub>* ，这样我们就可以预测明天的股价如下:

这是

# OpenCV 中的线性回归

在现实数据集上尝试线性回归之前，让我们了解如何使用`cv2.fitLine`函数将直线拟合到 2D 或三维点集:

1.  让我们从生成一些点开始。我们将通过给位于线上的点添加噪声来生成它们![](Images/25934214-92e9-4194-937c-af8f014804a7.png):

```py
In [1]: import cv2
...     import numpy as np
...     import matplotlib.pyplot as plt
...     from sklearn import linear_model
...     from sklearn.model_selection import train_test_split
...     plt.style.use('ggplot')
...     %matplotlib inline
In [2]: x = np.linspace(0,10,100)
...     y_hat = x*5+5
...     np.random.seed(42)
...     y = x*5 + 20*(np.random.rand(x.size) - 0.5)+5
```

2.  我们还可以使用以下代码来可视化这些点:

```py
In [3]: plt.figure(figsize=(10, 6))
...     plt.plot(x, y_hat, linewidth=4)
...     plt.plot(x,y,'x')
...     plt.xlabel('x')
...     plt.ylabel('y')
```

这给了我们下图，红线是真实函数:

![](Images/93344671-1736-4539-991e-8face2c9f3a6.png)

3.  接下来，我们将把这些点分成训练集和测试集。在这里，我们将数据分成 70:30 的比例，这意味着 70%的分数将用于培训，30%用于测试:

```py
In [4]: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)
```

4.  现在，让我们使用`cv2.fitLine`将一条线拟合到这个 2D 点集。此函数接受以下参数:
    *   `points`:这是一条线必须拟合的一组点。
    *   `distType`:这是 M 估计器使用的距离。
    *   `param`:这是数值参数(C)，用于某些类型的距离。我们将它保持在 0，以便可以选择一个最佳值。
    *   `reps`:这是原点到直线距离的精度。`0.01`对于`reps`来说是一个不错的默认值。
    *   `aeps`:这是角度的精度。`0.01`对于`aeps`来说是一个不错的默认值。

For more information, have a look at the [documentation](https://docs.opencv.org/4.0.0/d3/dc0/group__imgproc__shape.html#gaf849da1fdafa67ee84b1e9a23b93f91f).

5.  让我们看看使用不同的距离类型选项会得到什么样的结果:

```py
In [5]: distTypeOptions = [cv2.DIST_L2,\
...                 cv2.DIST_L1,\
...                 cv2.DIST_L12,\
...                 cv2.DIST_FAIR,\
...                 cv2.DIST_WELSCH,\
...                 cv2.DIST_HUBER]

In [6]: distTypeLabels = ['DIST_L2',\
...                 'DIST_L1',\
...                 'DIST_L12',\
...                 'DIST_FAIR',\
...                 'DIST_WELSCH',\
...                 'DIST_HUBER']

In [7]: colors = ['g','c','m','y','k','b']
In [8]: points = np.array([(xi,yi) for xi,yi in zip(x_train,y_train)])
```

6.  我们还将使用 scikit-learn 的`LinearRegression`来拟合训练点，然后使用`predict`功能来预测它们的 *y* 值:

```py
In [9]: linreg = linear_model.LinearRegression()
In [10]: linreg.fit(x_train.reshape(-1,1),y_train.reshape(-1,1))
Out[10]:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,normalize=False)
In [11]: y_sklearn = linreg.predict(x.reshape(-1,1))
In [12]: y_sklearn = list(y_sklearn.reshape(1,-1)[0])
```

7.  我们使用`reshape(-1,1)`和`reshape(1,-1)`将 NumPy 数组转换成列向量，然后再转换回行向量:

```py
In [13]: plt.figure(figsize=(10, 6))
...      plt.plot(x, y_hat,linewidth=2,label='Ideal')
...      plt.plot(x,y,'x',label='Data')

...      for i in range(len(colors)):
...          distType = distTypeOptions[i]
...          distTypeLabel = distTypeLabels[i]
...          c = colors[i]

...          [vxl, vyl, xl, yl] = cv2.fitLine(np.array(points, dtype=np.int32), distType, 0, 0.01, 0.01)
...          y_cv = [vyl[0]/vxl[0] * (xi - xl[0]) + yl[0] for xi in x]
...          plt.plot(x,y_cv,c=c,linewidth=2,label=distTypeLabel)

...      plt.plot(x,list(y_sklearn),c='0.5',\
linewidth=2,label='Scikit-Learn API')
...      plt.xlabel('x')
...      plt.ylabel('y')
...      plt.legend(loc='upper left')
```

前面这段(也是很长的)代码的唯一目的是创建一个图表，用于比较使用不同距离测量获得的结果。

我们来看看剧情:

![](Images/02196ce1-87a5-4273-b2b5-43bd4f62b56e.png)

我们可以清楚地看到，scikit-learn 的`LinearRegression`模型比 OpenCV 的`fitLine`功能表现要好很多。现在，让我们用 scikit-learn 的 API 来预测波士顿的房价。

# 用线性回归预测波士顿房价

为了更好地理解线性回归，我们希望构建一个简单的模型，该模型可以应用于最著名的机器学习数据集之一:**波士顿房价数据集**。在这里，目标是预测 20 世纪 70 年代波士顿几个街区的房屋价值，使用的信息包括犯罪率、财产税税率、到就业中心的距离和高速公路通达性。

# 正在加载数据集

我们可以再次感谢 scikit-learn 轻松访问数据集。我们首先导入所有必要的模块，就像之前一样:

```py
In [14]: from sklearn import datasets
...      from sklearn import metrics
```

然后加载数据集是一个单行过程:

```py
In [15]: boston = datasets.load_boston()
```

`boston`对象的结构与`iris`对象相同，如前面的命令所述。我们可以在`'DESCR'`获取更多数据集信息，并在`'data'`中找到所有数据，`'feature_names'`中找到所有要素名称，`'filename'`中找到波士顿 CSV 数据集的物理位置，`'target'`中找到所有目标值:

```py
In [16]: dir(boston)
Out[16]: ['DESCR', 'data', 'feature_names', 'filename', 'target']
```

数据集总共包含`506`个数据点，每个数据点都有`13`特征:

```py
In [17]: boston.data.shape
Out[17]: (506, 13)
```

当然，我们只有一个目标值，那就是房价:

```py
In [18]: boston.target.shape
Out[18]: (506,)
```

# 训练模型

现在让我们创建一个`LinearRegression`模型，然后在训练集上进行训练:

```py
In [19]: linreg = linear_model.LinearRegression()
```

在前面的命令中，我们希望将数据分成训练集和测试集。我们可以按照我们认为合适的方式进行拆分，但通常情况下，保留 10%到 30%用于测试是一个好主意。这里，我们选择 10%，使用`test_size`参数:

```py
In [20]: X_train, X_test, y_train, y_test = train_test_split(...            boston.data, boston.target, test_size=0.1,...            random_state=42...      )
```

在 scikit-learn 中，`train`函数被称为`fit`，但在其他方面的表现与 OpenCV 中完全相同:

```py
In [21]: linreg.fit(X_train, y_train)Out[21]: LinearRegression(copy_X=True, fit_intercept=True, ...
```

# 测试模型

为了测试模型的泛化性能，我们计算测试数据的均方误差:

```py
In [24]: y_pred = linreg.predict(X_test)
In [25]: metrics.mean_squared_error(y_test, y_pred)
Out[25]: 14.995852876582541
```

我们注意到测试集的均方误差比训练集稍低。这是个好消息，因为我们最关心的是测试错误。然而，从这些数字中，真的很难理解这个模型到底有多好。也许绘制数据会更好:

```py
In [26]: plt.figure(figsize=(10, 6))
...      plt.plot(y_test, linewidth=3, label='ground truth')
...      plt.plot(y_pred, linewidth=3, label='predicted')
...      plt.legend(loc='best')
...      plt.xlabel('test data points')
...      plt.ylabel('target value')
Out[26]: <matplotlib.text.Text at 0x7ff46783c7b8>
```

这产生了以下图表:

![](Images/2c458025-ac75-4430-91a7-23f68787efb4.png)

这更有道理！这里，我们看到所有测试样本的`ground truth`房价为红色，我们预测的房价为蓝色。如果你问我的话，这很近。不过，有趣的是，对于真正高或真正低的房价，该模型往往偏离得最多，例如数据点 **12** 、 **18** 和 **42** 的峰值。我们可以通过计算 R 的平方来形式化我们能够解释的数据中的方差量:

```py
In [27]: plt.figure(figsize=(10, 6))
...      plt.plot(y_test, y_pred, 'o')
...      plt.plot([-10, 60], [-10, 60], 'k--')
...      plt.axis([-10, 60, -10, 60])
...      plt.xlabel('ground truth')
...      plt.ylabel('predicted')
```

这将在 *x* 轴上绘制`ground truth`价格`y_test`，在 *y* 轴上绘制`y_pred`预测。我们还绘制了一条对角线作为参考(使用黑色虚线，`'k--'`)，我们很快就会看到。但是我们也想在文本框中显示 R <sup>2</sup> 分数和均方误差:

```py
...      scorestr = r'R$^2$ = %.3f' % linreg.score(X_test, y_test)
...      errstr = 'MSE = %.3f' % metrics.mean_squared_error(y_test, y_pred)
...      plt.text(-5, 50, scorestr, fontsize=12)
...      plt.text(-5, 45, errstr, fontsize=12)
Out[27]: <matplotlib.text.Text at 0x7ff4642d0400>
```

这将生成下图，并且是绘制模型拟合的专业方法:

![](Images/139e4726-1300-4c73-aa11-c8282dba5832.png)

如果我们的模型是完美的，那么所有的数据点将位于虚线对角线上，因为`y_pred`将总是等于`y_true`。与对角线的偏差表明模型犯了一些错误，或者模型无法解释的数据中存在一些差异。实际上，![](Images/a15dc10c-dde6-4547-ac44-71066ff64510.png)表明我们能够解释数据中 76%的分散，均方误差为 14.996。这些是我们可以用来比较线性回归模型和一些更复杂的模型的一些性能度量。

# 应用套索和岭回归

机器学习中的一个常见问题是，一个算法可能在训练集上运行得很好，但是，当应用于看不见的数据时，它会犯很多错误。你可以看到这是多么的有问题，因为，通常，我们最感兴趣的是模型如何推广到新的数据。一些算法(如决策树)比其他算法更容易受到这种现象的影响，但甚至线性回归也会受到影响。

This phenomenon is also known as **overfitting**, and we will talk about it extensively in [Chapter 5](05.html), *Using Decision Trees to Make a Medical Diagnosis*, and [Chapter 11](11.html), *Selecting the Right Model with Hyperparameter Tuning*.

减少过拟合的常用技术称为**正则化**，包括...

# 利用逻辑回归对鸢尾属植物进行分类

机器学习领域另一个著名的数据集叫做 Iris 数据集。鸢尾数据集包含来自三个不同物种的 150 朵鸢尾花的测量值:濑户鸢尾、云芝和绿鸢尾。这些尺寸包括花瓣的长度和宽度以及萼片的长度和宽度，都以厘米为单位。

我们的目标是建立一个机器学习模型，可以学习这些鸢尾花的测量值，它们的种类是已知的，这样我们就可以预测新鸢尾花的种类。

# 理解逻辑回归

在我们开始这一部分之前，让我发出警告——逻辑回归，不管它的名字是什么，实际上是一个分类模型，特别是当你有两个类的时候。它的名称来源于逻辑函数(或 sigmoid ),该函数用于将任何实值输入 *x* 转换为预测输出值 *ŷ* ,其取值介于 **0** 和 **1** 之间，如下图所示:

![](Images/b9b54eb8-287c-4286-a68e-d5f9496af292.png)

将 *ŷ* 舍入到最接近的整数有效地将输入分类为属于类别 **0** 或 **1** 。

当然，大多数情况下，我们的问题有不止一个输入或特征值， *x* 。例如，Iris 数据集提供了一个总数...

# 加载训练数据

Iris 数据集包含在 scikit-learn 中。我们首先加载所有必要的模块，正如我们在前面的例子中所做的:

```py
In [1]: import numpy as np
...     import cv2
...     from sklearn import datasets
...     from sklearn import model_selection
...     from sklearn import metrics
...     import matplotlib.pyplot as plt
...     %matplotlib inline
In [2]: plt.style.use('ggplot')
```

然后加载数据集是一个单行过程:

```py
In [3]: iris = datasets.load_iris()
```

这个函数返回一个我们称之为`iris`的字典，它包含一堆不同的字段:

```py
In [4]: dir(iris)
Out[4]: ['DESCR', 'data', 'feature_names', 'filename', 'target', 'target_names']
```

这里所有的数据点都包含在`'data'`中。有`150`个数据点，每个数据点都有`4`特征值:

```py
In [5]: iris.data.shape
Out[5]: (150, 4)
```

这四个特征对应于前面提到的萼片和花瓣尺寸:

```py
In [6]: iris.feature_names
Out[6]: ['sepal length (cm)',
         'sepal width (cm)',
         'petal length (cm)',
         'petal width (cm)']
```

对于每个数据点，我们都有一个存储在`target`中的类标签:

```py
In [7]: iris.target.shape
Out[7]: (150,)
```

我们还可以检查类别标签，发现总共有三个类别:

```py
In [8]: np.unique(iris.target)
Out[8]: array([0, 1, 2])
```

# 使它成为一个二元分类问题

为了简单起见，我们现在想集中讨论一个二元分类问题，这里我们只有两个类。最简单的方法是通过选择所有不属于类`2`的行来丢弃属于某个类的所有数据点，如类标签 2:

```py
In [9]: idx = iris.target != 2...     data = iris.data[idx].astype(np.float32)...     target = iris.target[idx].astype(np.float32)
```

接下来，让我们检查数据。

# 检查数据

在开始建立模型之前，看一下数据总是一个好主意。我们之前在城镇地图的例子中做过，所以我们也在这里重复一下。使用 Matplotlib，我们创建一个**散点图**，其中每个数据点的颜色对应于类别标签:

```py
In [10]: plt.scatter(data[:, 0], data[:, 1], c=target,  
                     cmap=plt.cm.Paired, s=100)
...      plt.xlabel(iris.feature_names[0])
...      plt.ylabel(iris.feature_names[1])
Out[10]: <matplotlib.text.Text at 0x23bb5e03eb8>
```

为了使绘图更容易，我们将自己限制在前两个特征上(`iris.feature_names[0]`是萼片长度，`iris.feature_names[1]`是萼片宽度)。我们可以在下图中看到很好的类分离:

![](Images/02b2daa3-8a42-4601-b8c5-e84f87ed0cdb.png)

上图显示了虹膜数据集的前两个特征。

# 将数据分成训练集和测试集

在上一章中，我们了解到将训练数据和测试数据分开是非常重要的。我们可以使用 scikit-learn 的许多辅助函数之一轻松拆分数据:

```py
In [11]: X_train, X_test, y_train, y_test = model_selection.train_test_split(...            data, target, test_size=0.1, random_state=42...      )
```

这里我们要把数据拆分成 90%的训练数据和 10%的测试数据，我们用`test_size=0.1`指定。通过检查返回参数，我们注意到我们最终得到了精确的`90`训练数据点和`10`测试数据点:

```py
In [12]: X_train.shape, y_train.shapeOut[12]: ((90, 4), (90,))In [13]: X_test.shape, y_test.shapeOut[13]: ((10, 4), (10,))
```

# 训练分类器

创建逻辑回归分类器的步骤与设置 k-NN 的步骤基本相同:

```py
In [14]: lr = cv2.ml.LogisticRegression_create()
```

然后我们必须指定期望的训练方法。这里我们可以选择`cv2.ml.LogisticRegression_BATCH`或者`cv2.ml.LogisticRegression_MINI_BATCH`。目前，我们需要知道的是，我们希望在每个数据点之后更新模型，这可以通过以下代码来实现:

```py
In [15]: lr.setTrainMethod(cv2.ml.LogisticRegression_MINI_BATCH)
...      lr.setMiniBatchSize(1)
```

我们还希望指定算法在终止之前应该运行的迭代次数:

```py
In [16]: lr.setIterations(100)
```

然后，我们可以调用对象的`train`方法(与我们之前的方法完全相同)，该方法将在成功时返回`True`:

```py
In [17]: lr.train(X_train, cv2.ml.ROW_SAMPLE, y_train)
Out[17]: True
```

正如我们刚刚看到的，训练阶段的目标是找到一组权重，最好地将特征值转换为输出标签。单个数据点由其四个特征值给出(*f<sub>0</sub>T3、*f<sub>1</sub>T7、*f<sub>2</sub>T11、*f<sub>3</sub>T15】)。既然我们有四个特征，那么我们也应该得到四个权重，这样*x = w<sub>0</sub>f<sub>0</sub>+w<sub>1</sub>f<sub>1</sub>+w<sub>2</sub>f<sub>2</sub>+w<sub>3</sub>f<sub>3</sub>t33】和 *ŷ=σ(x)* 。然而，如前所述，该算法增加了一个额外的权重，作为偏移或偏差，从而使*x = w<sub>0</sub>f<sub>0</sub>+w<sub>1</sub>f<sub>1</sub>+w<sub>2</sub>f<sub>2</sub>+w<sub>3</sub>f<sub>3</sub>+w<sub>4】</sub>*。我们可以按如下方式检索这些权重:*****

```py
In [18]: lr.get_learnt_thetas()
Out[18]: array([[-0.04090132, -0.01910266, -0.16340332, 0.28743777, 0.11909772]], dtype=float32)
```

这意味着逻辑函数的输入为*x =-0.0409 f<sub>0</sub>-0.0191 f<sub>1</sub>-0.163 f<sub>2</sub>+0.287 f<sub>3</sub>+0.119*。然后，当我们馈入属于类 1 的新数据点( *f <sub>0</sub>* 、 *f <sub>1</sub>* 、 *f <sub>2</sub>* 、 *f <sub>3</sub>* )时，输出 *ŷ=σ(x)* 应该接近 1。但这实际上有多有效呢？

# 测试分类器

让我们通过计算训练集上的准确度分数来亲眼看看:

```py
In [19]: ret, y_pred = lr.predict(X_train)In [20]: metrics.accuracy_score(y_train, y_pred)Out[20]: 1.0
```

满分！然而，这仅仅意味着模型能够完美地**记忆**训练数据集。这并不意味着模型能够对一个新的、看不见的数据点进行分类。为此，我们需要检查测试数据集:

```py
In [21]: ret, y_pred = lr.predict(X_test)...      metrics.accuracy_score(y_test, y_pred)Out[21]: 1.0
```

幸运的是，我们又得了一个满分！现在我们可以肯定，我们建立的模型真的很棒。

# 摘要

在这一章里，我们谈了很多，不是吗？

简而言之，我们学习了很多不同的监督学习算法，如何将它们应用于真实数据集，以及如何在 OpenCV 中实现一切。我们介绍了分类算法，如 k-NN 和逻辑回归，并讨论了它们如何用于预测两个或多个离散类别的标签。我们介绍了线性回归的各种变体(如套索回归和岭回归)，并讨论了它们如何用于预测连续变量。最后但同样重要的是，我们熟悉了 Iris 和 Boston 数据集，这是机器学习历史上的两个经典。

在接下来的章节中，我们将深入探讨这些主题，并探索一些更有趣的例子来说明这些概念的用处。

但是首先，我们需要谈谈机器学习中的另一个基本话题，特征工程。通常，数据不会出现在格式良好的数据集中，以有意义的方式表示数据是我们的责任。因此，下一章将讨论表示特征和工程数据。